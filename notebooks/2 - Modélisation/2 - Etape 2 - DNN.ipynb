{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c6e350-cd17-40ca-bc19-4797dc43cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 12:25:25.429105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_scorer, f1_score, recall_score\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report                # Performance accuracy, f1-score\n",
    "from sklearn.metrics import roc_auc_score                        # performance en utilisant la courbe ROC et le score AUC\n",
    "from sklearn.metrics import log_loss                             # Perte logarithmique\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Chemins d'accès aux données\n",
    "repDataConsolidees = '../../data/processed/'\n",
    "repModels = '../../models/'\n",
    "\n",
    "# Liste des fichiers usagers disponibles\n",
    "!pwd\n",
    "!ls {repDataConsolidees}*  -lrt\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d89656-f154-4c12-832f-ee3ee7ff101e",
   "metadata": {},
   "source": [
    "### CHARGEMENT DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea80edb-cd78-45c7-a35a-c97da6a25ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension données d'apprentissage, nombre de lignes= 270717 nombre de features= 177\n",
      "Dimension données de test, nombre de lignes= 90273 nombre de features= 177\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'apprentissage :\n",
      "- X_train_multimodal_resampled :  (456004, 177)\n",
      "- X_train_binary_resampled :  (444680, 177)\n",
      "- y_train_multimodal_resampled :  (456004,)\n",
      "- y_train_binary_resampled :  (444680,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Données d'apprentissage\n",
    "X_train = pd.read_csv(repDataConsolidees + 'X_train_preprocessed.csv')\n",
    "y_train_multimodal = pd.read_csv(repDataConsolidees + 'y_train_preprocessed.csv')\n",
    "y_train_binary = pd.read_csv(repDataConsolidees + 'y_train_binary_preprocessed.csv')\n",
    "\n",
    "# Données de test\n",
    "X_test = pd.read_csv(repDataConsolidees + 'X_test_preprocessed.csv')\n",
    "y_test_multimodal = pd.read_csv(repDataConsolidees + 'y_test_preprocessed.csv')\n",
    "y_test_binary = pd.read_csv(repDataConsolidees + 'y_test_binary_preprocessed.csv')\n",
    "\n",
    "# Nombre d'inputs :\n",
    "print (\"Dimension données d'apprentissage, nombre de lignes=\", X_train.shape[0], \"nombre de features=\", X_train.shape[1])\n",
    "print (\"Dimension données de test, nombre de lignes=\", X_test.shape[0], \"nombre de features=\", X_test.shape[1])\n",
    "\n",
    "# Rééquilibrage des données avec SMOTE\n",
    "smote_multimodal = SMOTE(random_state=42)\n",
    "smote_binary = SMOTE(random_state=42)\n",
    "X_train_multimodal_resampled, y_train_multimodal_resampled = smote_multimodal.fit_resample(X_train, y_train_multimodal)\n",
    "X_train_binary_resampled, y_train_binary_resampled = smote_binary.fit_resample(X_train, y_train_binary)\n",
    "y_train_multimodal_resampled = y_train_multimodal_resampled['user_gravite']       # passe de df --> vecteur\n",
    "y_train_binary_resampled = y_train_binary_resampled['user_gravite']\n",
    "\n",
    "# Contrôle répartition des données multinomiale\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Diagramme pour y_train_multi\n",
    "sns.countplot(x=y_train_multimodal_resampled, ax=axes[0])\n",
    "axes[0].set_title('Répartition de y_train_multimodal_resampled (SMOTE)')\n",
    "axes[0].set_xlabel('Catégories')\n",
    "axes[0].set_ylabel('Fréquence')\n",
    "\n",
    "# Diagramme pour y_train_binary\n",
    "sns.countplot(x=y_train_binary_resampled, ax=axes[1])\n",
    "axes[1].set_title('Répartition de y_train_binary_resampled (SMOTE)')\n",
    "axes[1].set_xlabel('Classes')\n",
    "axes[1].set_ylabel('Fréquence')\n",
    "\n",
    "# Ajuster les espacements entre les sous-graphes\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print (\"Données d'apprentissage :\")\n",
    "print (\"- X_train_multimodal_resampled : \", X_train_multimodal_resampled.shape )\n",
    "print (\"- X_train_binary_resampled : \", X_train_binary_resampled.shape )\n",
    "print (\"- y_train_multimodal_resampled : \", y_train_multimodal_resampled.shape)\n",
    "print (\"- y_train_binary_resampled : \", y_train_binary_resampled.shape)\n",
    "print (\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0020c13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "213e6264",
   "metadata": {},
   "source": [
    "### CALLBACK - DECLARATION DES FONCTIONS UTILISEES DANS CHAQUE RESEAU TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004aab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "# Arrêt de l'apprentissage lorsque la métrique surveillée ne s'améliore plus à partir d'un certain nombre d'époque\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Sauvegarde des points afin de récupérer le modèle le plus performant\n",
    "checkpoint = ModelCheckpoint('model_best.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Ajustement du taux d'apprentissage afin d'améliorer la convergence \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tensorflow.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac304d32",
   "metadata": {},
   "source": [
    "<u>TEST 0 - REFERENCE - DNN - CLASSIFICATION MULTICLASSES - SANS REEQUILIBRAGE - METRIQUE=ACCURACY</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a034c55-d46e-4e8e-8a4a-ea6bb3d59736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 159)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1600      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,770\n",
      "Trainable params: 1,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# TEST 0 - CLASSIFICATION MULTIMODALE - SANS REEQUILIBRAGE - METRIQUE=ACCURACY\n",
    "# ----------------------------------------------------------------------------\n",
    "# TEST0.1. CONSTRUCTION DU MODELE :\n",
    "# ---------------------------------\n",
    "# Construction d'un DNN sur la base d'une structure similaire à celle du cours \n",
    "# (Difficile de définir la structure optimale qui répondra le mieux à notre problème)\n",
    "# Réseau configuré pour de la classification multimodales (maintien des 4 classes cibles)\n",
    "# Ajout de couche dropout après chaque couche Dense afin d'éviter l'overfitting\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 6, activation = \"tanh\", name = \"Dense_3\")\n",
    "Dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "# TEST 0.2 - COMPILATION DU MODELE - Classification multi classes sans rééquilibrage - fction éval=accuracy\n",
    "# ---------------------------------\n",
    "# S'agissant d'un problème de classification multi classes, la fonction de perte privilégiée\n",
    "# sera : sparse_categorical_crossentropy\n",
    "#\n",
    "# 1. Maintien de la métrique accuracy pour évaluer les performances du modèle\n",
    "# ------------------------\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer = \"adam\",\n",
    "              metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "# TEST 0.3 - APPRENTISSAGE DU MODELE (classification multiclasse - accuracy - SANS REEQUILIBRAGE)\n",
    "# -----------------------------------\n",
    "# Résultats bruts pour base de comparaison avec les 3 tests suivants\n",
    "\n",
    "history_test0 = model.fit(X_train, y_train_multimodal, epochs=50, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test0.history['loss']\n",
    "train_accuracy_history = history_test0.history['accuracy']\n",
    "\n",
    "# Récupération des scores de validation\n",
    "val_loss_history = history_test0.history.get('val_loss')\n",
    "val_accuracy_history = history_test0.history.get('val_accuracy')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f9dadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.72      0.84      0.77     38039\n",
      "           1       0.66      0.58      0.62     35848\n",
      "           2       0.47      0.48      0.48     13946\n",
      "           3       0.44      0.02      0.03      2354\n",
      "\n",
      "    accuracy                           0.66     90273\n",
      "   macro avg       0.46      0.38      0.38     90273\n",
      "weighted avg       0.65      0.66      0.65     90273\n",
      "\n",
      "[[    0    83     3     0     0]\n",
      " [    0 31992  4560  1484     3]\n",
      " [    0 10425 20908  4500    15]\n",
      " [    0  1861  5391  6658    36]\n",
      " [    0   237   641  1434    42]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 0.4 - EVALUATION DU MODELE (classification multiclasse - accuracy - SANS REEQUILIBRAGE)\n",
    "# -------------------------------\n",
    "# Résultats bruts pour base de comparaison avec les 3 tests suivants\n",
    "\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n",
    "# On constate l'effet du déséquilibre sur le modèle qui apprend correctement les classes majoritaire au détriement des autres dont la classe 3 qui nous intéresse.\n",
    "#               precision    recall  f1-score   support\n",
    "'''\n",
    "          -1       0.00      0.00      0.00        86\n",
    "           0       0.72      0.84      0.77     38039\n",
    "           1       0.66      0.58      0.62     35848\n",
    "           2       0.47      0.48      0.48     13946\n",
    "           3       0.44      0.02      0.03      2354\n",
    "\n",
    "    accuracy                           0.66     90273\n",
    "   macro avg       0.46      0.38      0.38     90273\n",
    "weighted avg       0.65      0.66      0.65     90273\n",
    "\n",
    "[[    0    83     3     0     0]\n",
    " [    0 31992  4560  1484     3]\n",
    " [    0 10425 20908  4500    15]\n",
    " [    0  1861  5391  6658    36]\n",
    " [    0   237   641  1434    42]]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890b686",
   "metadata": {},
   "source": [
    "<u>TEST 1 - CLASSIFICATION MULTICLASSES - METRIQUE=accuracy - LOSS=SPARSE_CATEGORICAL_ENTROPY - SMOTE</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93a46f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1780      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,950\n",
      "Trainable params: 1,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12826/12826 [==============================] - 35s 3ms/step - loss: 1.0488 - accuracy: 0.5282 - val_loss: 1.2336 - val_accuracy: 0.2849 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12826/12826 [==============================] - 24s 2ms/step - loss: 1.0228 - accuracy: 0.5460 - val_loss: 1.2071 - val_accuracy: 0.3648 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12826/12826 [==============================] - 24s 2ms/step - loss: 1.0196 - accuracy: 0.5492 - val_loss: 1.2283 - val_accuracy: 0.3926 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12826/12826 [==============================] - 24s 2ms/step - loss: 1.0174 - accuracy: 0.5495 - val_loss: 1.1872 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12826/12826 [==============================] - 24s 2ms/step - loss: 1.0149 - accuracy: 0.5495 - val_loss: 1.1937 - val_accuracy: 0.3846 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12826/12826 [==============================] - 27s 2ms/step - loss: 1.0156 - accuracy: 0.5502 - val_loss: 1.1786 - val_accuracy: 0.3863 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12826/12826 [==============================] - 27s 2ms/step - loss: 1.0141 - accuracy: 0.5501 - val_loss: 1.0634 - val_accuracy: 0.5489 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12826/12826 [==============================] - 24s 2ms/step - loss: 1.0130 - accuracy: 0.5507 - val_loss: 1.2676 - val_accuracy: 0.3562 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12826/12826 [==============================] - 24s 2ms/step - loss: 1.0123 - accuracy: 0.5508 - val_loss: 1.1618 - val_accuracy: 0.4519 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12826/12826 [==============================] - 25s 2ms/step - loss: 1.0118 - accuracy: 0.5513 - val_loss: 1.2589 - val_accuracy: 0.2814 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 1.0118072032928467\n",
      "Exactitude finale sur le jeu d'entraînement: 0.5512849688529968\n",
      "Perte finale sur le jeu de validation: 1.2589006423950195\n",
      "Exactitude finale sur le jeu de validation: 0.28139734268188477\n"
     ]
    }
   ],
   "source": [
    "# TEST 1 - Modèle pour Classification multi classes - SMOTE - accuracy, la fonction de perte privilégiée\n",
    "# sera : sparse_categorical_crossentropy\n",
    "#\n",
    "# TEST 1.1. CONSTRUCTION DU MODELE :\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 6, activation = \"tanh\", name = \"Dense_3\")\n",
    "Dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "# TEST 1.2. COMPILATION DU MODELE (Classification multiclasses - METRIQUE=accuracy - LOSS=SPARSE_CATEGORICAL_ENTROPY - SMOTE)\n",
    "# ------------------------\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer = \"adam\",\n",
    "              metrics=[\"accuracy\"]\n",
    "             )\n",
    "\n",
    "\n",
    "# TEST 1.3 - APPRENTISSAGE DU MODELE (classification multiclasse - SMOTE - accuracy)\n",
    "# ----------------------------------\n",
    "history_test1 = model.fit(\n",
    "    X_train_multimodal_resampled, \n",
    "    y_train_multimodal_resampled, \n",
    "    validation_split=0.1,\n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test1.history['loss']\n",
    "train_accuracy_history = history_test1.history['accuracy']\n",
    "\n",
    "# Récupération des scores de validation\n",
    "val_loss_history = history_test1.history.get('val_loss')\n",
    "val_accuracy_history = history_test1.history.get('val_accuracy')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c8fb862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14251/14251 [==============================] - 27s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40    114001\n",
      "           1       0.00      0.00      0.00    114001\n",
      "           2       0.00      0.00      0.00    114001\n",
      "           3       0.00      0.00      0.00    114001\n",
      "\n",
      "    accuracy                           0.25    456004\n",
      "   macro avg       0.06      0.25      0.10    456004\n",
      "weighted avg       0.06      0.25      0.10    456004\n",
      "\n",
      "[[114001      0      0      0]\n",
      " [114001      0      0      0]\n",
      " [114001      0      0      0]\n",
      " [114001      0      0      0]]\n",
      "2822/2822 [==============================] - 5s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.42      1.00      0.59     38039\n",
      "           1       0.00      0.00      0.00     35848\n",
      "           2       0.00      0.00      0.00     13946\n",
      "           3       0.00      0.00      0.00      2354\n",
      "\n",
      "    accuracy                           0.42     90273\n",
      "   macro avg       0.08      0.20      0.12     90273\n",
      "weighted avg       0.18      0.42      0.25     90273\n",
      "\n",
      "[[    0    86     0     0     0]\n",
      " [    0 38039     0     0     0]\n",
      " [    0 35848     0     0     0]\n",
      " [    0 13946     0     0     0]\n",
      " [    0  2354     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n              precision    recall  f1-score   support\\n\\n          -1       0.00      0.00      0.00        86\\n           0       0.76      0.74      0.75     38039\\n           1       0.66      0.50      0.57     35848\\n           2       0.36      0.46      0.40     13946\\n           3       0.14      0.48      0.22      2354\\n\\n    accuracy                           0.59     90273\\n   macro avg       0.38      0.44      0.39     90273\\nweighted avg       0.64      0.59      0.61     90273\\n\\n[[    0    81     1     3     1]\\n [    0 28122  6005  2734  1178]\\n [    0  7824 17954  7730  2340]\\n [    0  1065  2999  6397  3485]\\n [    0   118   293   812  1131]]\\n\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 1.4 - EVALUATION DU MODELE (classification multiclasse - accuracy - Rééquilibrage SMOTE)\n",
    "# -------------------------------\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#y_train_pred=model.predict(X_train)\n",
    "#y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "y_train_pred=model.predict(X_train_multimodal_resampled)\n",
    "y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_multimodal_resampled, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_multimodal_resampled,y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n",
    "'''\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "          -1       0.00      0.00      0.00        86\n",
    "           0       0.76      0.74      0.75     38039\n",
    "           1       0.66      0.50      0.57     35848\n",
    "           2       0.36      0.46      0.40     13946\n",
    "           3       0.14      0.48      0.22      2354\n",
    "\n",
    "    accuracy                           0.59     90273\n",
    "   macro avg       0.38      0.44      0.39     90273\n",
    "weighted avg       0.64      0.59      0.61     90273\n",
    "\n",
    "[[    0    81     1     3     1]\n",
    " [    0 28122  6005  2734  1178]\n",
    " [    0  7824 17954  7730  2340]\n",
    " [    0  1065  2999  6397  3485]\n",
    " [    0   118   293   812  1131]]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91470d7b",
   "metadata": {},
   "source": [
    "<u>TEST 2 - CLASSIFICATION MULTICLASSE - METRIQUE=F1-SCORE - LOSS=SPARSE_CATEGORIAL_ENTROPY - SMOTE</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ecbf3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1780      \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,950\n",
      "Trainable params: 1,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12826/12826 [==============================] - 38s 3ms/step - loss: 1.0495 - f1_score: 0.0295 - val_loss: 1.3029 - val_f1_score: 0.0072 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12826/12826 [==============================] - 42s 3ms/step - loss: 1.0235 - f1_score: 0.0296 - val_loss: 1.2890 - val_f1_score: 0.0064 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12826/12826 [==============================] - 35s 3ms/step - loss: 1.0203 - f1_score: 0.0296 - val_loss: 1.2177 - val_f1_score: 0.0129 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12826/12826 [==============================] - 26s 2ms/step - loss: 1.0174 - f1_score: 0.0298 - val_loss: 1.2006 - val_f1_score: 0.0174 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12826/12826 [==============================] - 25s 2ms/step - loss: 1.0156 - f1_score: 0.0298 - val_loss: 1.2201 - val_f1_score: 0.0173 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12826/12826 [==============================] - 26s 2ms/step - loss: 1.0145 - f1_score: 0.0298 - val_loss: 1.1471 - val_f1_score: 0.0202 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12826/12826 [==============================] - 22s 2ms/step - loss: 1.0113 - f1_score: 0.0298 - val_loss: 1.2051 - val_f1_score: 0.0172 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12826/12826 [==============================] - 23s 2ms/step - loss: 1.0104 - f1_score: 0.0298 - val_loss: 1.1136 - val_f1_score: 0.0204 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12826/12826 [==============================] - 26s 2ms/step - loss: 1.0097 - f1_score: 0.0298 - val_loss: 1.1916 - val_f1_score: 0.0181 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12826/12826 [==============================] - 27s 2ms/step - loss: 1.0096 - f1_score: 0.0298 - val_loss: 1.2258 - val_f1_score: 0.0170 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12826/12826 [==============================] - 26s 2ms/step - loss: 1.0070 - f1_score: 0.0298 - val_loss: 1.2012 - val_f1_score: 0.0163 - lr: 9.0484e-04\n",
      "Perte finale sur le jeu d'entraînement: 1.0069891214370728\n",
      "Exactitude finale sur le jeu d'entraînement: 0.029840538278222084\n",
      "Perte finale sur le jeu de validation: 1.2012089490890503\n",
      "Exactitude finale sur le jeu de validation: 0.016328115016222\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 2 -  métrique f1-score avec classifivation multiclasses \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 2.1 - CONSTRUCTION DU MODELE (Classification multiclasse - SMOTE - f1-Score)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 6, activation = \"tanh\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 2.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score\n",
    "# ------------------------------\n",
    "# Définition du F1-score multiclasses\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Convertir en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=tf.shape(y_pred)[-1])\n",
    "\n",
    "    # Calcul des true positives, predicted positives et possible positives\n",
    "    true_positives = tf.reduce_sum(tf.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = tf.reduce_sum(tf.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)  # Remplacer les NaN par des zéros\n",
    "\n",
    "    return tf.reduce_mean(f1)  # Moyenne sur toutes les classes\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              #metrics=[lambda y_true, y_pred: weighted_f1_score(y_true, y_pred, class_weights)])\n",
    "              metrics=[f1_score])\n",
    "\n",
    "# TEST 2.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score)\n",
    "# -----------------------------------\n",
    "history_test2 = model.fit(\n",
    "    X_train_multimodal_resampled, \n",
    "    y_train_multimodal_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test2.history['loss']\n",
    "train_accuracy_history = history_test2.history['f1_score']                   \n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test2.history.get('val_loss')\n",
    "val_accuracy_history = history_test2.history.get('val_f1_score')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e8fcb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 15s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.76    114001\n",
      "           1       0.71      0.42      0.53    108339\n",
      "           2       0.36      0.53      0.43     41182\n",
      "           3       0.16      0.45      0.24      7195\n",
      "\n",
      "    accuracy                           0.60    270717\n",
      "   macro avg       0.49      0.55      0.49    270717\n",
      "weighted avg       0.65      0.60      0.60    270717\n",
      "\n",
      "[[92036 10071  9132  2762]\n",
      " [30713 45413 26671  5542]\n",
      " [ 3592  7560 21678  8352]\n",
      " [  422   707  2845  3221]]\n",
      "2822/2822 [==============================] - 3s 913us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.73      0.81      0.77     38039\n",
      "           1       0.71      0.42      0.53     35848\n",
      "           2       0.36      0.53      0.43     13946\n",
      "           3       0.15      0.42      0.22      2354\n",
      "\n",
      "    accuracy                           0.60     90273\n",
      "   macro avg       0.39      0.43      0.39     90273\n",
      "weighted avg       0.65      0.60      0.60     90273\n",
      "\n",
      "[[    0    82     0     2     2]\n",
      " [    0 30711  3222  3096  1010]\n",
      " [    0 10119 14987  8890  1852]\n",
      " [    0  1201  2570  7359  2816]\n",
      " [    0   127   254   988   985]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 2.4 - EVALUATION DU MODELE (multiclasse - f1-Score)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_multimodal, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_multimodal,y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4273514",
   "metadata": {},
   "source": [
    "<u>TEST 3 - CLASSIFICATION MULTICLASSE - METRIQUE=F1-SCORE PONDERE - LOSS=SPARSE_CATEGORIAL_ENTROPY - SMOTE</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb2a9f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1780      \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 6)                 54        \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 28        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,950\n",
      "Trainable params: 1,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12826/12826 [==============================] - 65s 5ms/step - loss: 1.0508 - named_weighted_f1_score: 7.5750 - val_loss: 1.2112 - val_named_weighted_f1_score: 4.1690 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12826/12826 [==============================] - 35s 3ms/step - loss: 1.0227 - named_weighted_f1_score: 7.5996 - val_loss: 1.2371 - val_named_weighted_f1_score: 4.3576 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 1.0178 - named_weighted_f1_score: 7.6117 - val_loss: 1.1812 - val_named_weighted_f1_score: 4.4357 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 1.0164 - named_weighted_f1_score: 7.6217 - val_loss: 1.2538 - val_named_weighted_f1_score: 4.5327 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12826/12826 [==============================] - 36s 3ms/step - loss: 1.0156 - named_weighted_f1_score: 7.6276 - val_loss: 1.2804 - val_named_weighted_f1_score: 3.0623 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12826/12826 [==============================] - 38s 3ms/step - loss: 1.0147 - named_weighted_f1_score: 7.6305 - val_loss: 1.2314 - val_named_weighted_f1_score: 4.2551 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 1.0147099494934082\n",
      "Exactitude finale sur le jeu d'entraînement: 7.630532741546631\n",
      "Perte finale sur le jeu de validation: 1.2314082384109497\n",
      "Exactitude finale sur le jeu de validation: 4.255081653594971\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 3 -  métrique f1-score pondéré avec classifivation multiclasses \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 3.1 - CONSTRUCTION DU MODELE (Classification multiclasse - SMOTE - f1-Score pondéré)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 6, activation = \"tanh\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 3.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Utilisation d'une factory de fonction afin de permettre le passage en paramètre de class_weights\n",
    "\n",
    "def weighted_f1_score(y_true, y_pred, class_weights):\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "\n",
    "    # Convertir les labels en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=len(class_weights))\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=len(class_weights))\n",
    "\n",
    "    # Précision et rappel pour chaque classe\n",
    "    true_positives = K.sum(K.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = K.sum(K.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = K.sum(K.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    # F1-score par classe\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    # Conversion des poids en float32\n",
    "    class_weights = K.cast(class_weights, dtype='float32')\n",
    "\n",
    "    # Calcul du f1-score pondéré\n",
    "    weighted_f1 = K.sum(f1_scores * class_weights) / K.sum(class_weights)\n",
    "\n",
    "    return weighted_f1\n",
    "\n",
    "\n",
    "# Calcul des poids des classes\n",
    "unique_classes, class_counts = np.unique(y_train_multimodal_resampled, return_counts=True)\n",
    "class_weights = class_counts / len(y_train_multimodal_resampled)\n",
    "class_weights = 1.0 / class_weights\n",
    "\n",
    "# Création de la fonction weighted à partir de la factory afin de passer 2 des arguments lors de l'appel dans le compile, le 3ème (class_weights étant prés transmis via la factory\n",
    "def make_weighted_f1_score(class_weights):\n",
    "    def named_weighted_f1_score(y_true, y_pred):\n",
    "        return weighted_f1_score(y_true, y_pred, class_weights)\n",
    "    return named_weighted_f1_score\n",
    "\n",
    "weighted_f1 = make_weighted_f1_score(class_weights)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              #metrics=[lambda y_true, y_pred: weighted_f1_score(y_true, y_pred, class_weights)])\n",
    "              metrics=[weighted_f1])\n",
    "\n",
    "# TEST 3.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test3 = model.fit(\n",
    "    X_train_multimodal_resampled, \n",
    "    y_train_multimodal_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test3.history['loss']\n",
    "train_accuracy_history = history_test3.history['named_weighted_f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test3.history.get('val_loss')\n",
    "val_accuracy_history = history_test3.history.get('val_named_weighted_f1_score')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e76305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 19s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77    114001\n",
      "           1       0.72      0.39      0.51    108339\n",
      "           2       0.37      0.56      0.44     41182\n",
      "           3       0.18      0.36      0.24      7195\n",
      "\n",
      "    accuracy                           0.60    270717\n",
      "   macro avg       0.49      0.54      0.49    270717\n",
      "weighted avg       0.65      0.60      0.60    270717\n",
      "\n",
      "[[94900  8284  8905  1912]\n",
      " [33975 42617 28035  3712]\n",
      " [ 4369  7224 23247  6342]\n",
      " [  535   681  3387  2592]]\n",
      "2822/2822 [==============================] - 5s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.71      0.83      0.77     38039\n",
      "           1       0.72      0.39      0.51     35848\n",
      "           2       0.37      0.57      0.45     13946\n",
      "           3       0.16      0.34      0.22      2354\n",
      "\n",
      "    accuracy                           0.60     90273\n",
      "   macro avg       0.39      0.43      0.39     90273\n",
      "weighted avg       0.65      0.60      0.60     90273\n",
      "\n",
      "[[    0    81     1     3     1]\n",
      " [    0 31615  2707  3001   716]\n",
      " [    0 11132 14111  9360  1245]\n",
      " [    0  1469  2432  7910  2135]\n",
      " [    0   148   246  1156   804]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 3.4 - EVALUATION DU MODELE (multiclasse - f1-Score pondéré)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_multimodal, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_multimodal,y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6c928",
   "metadata": {},
   "source": [
    "<u>TEST 4 - CLASSIFICATION MULTICLASSE - METRIQUE=F1-SCORE - LOSS=SPARSE_CATEGORIAL_ENTROPY - SMOTE<u> \n",
    "\n",
    "Augmentation du nb de neurones par couche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a2958e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,954\n",
      "Trainable params: 60,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12826/12826 [==============================] - 76s 6ms/step - loss: 0.9800 - f1_score: 0.0300 - val_loss: 1.2310 - val_f1_score: 0.0176 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12826/12826 [==============================] - 60s 5ms/step - loss: 0.9408 - f1_score: 0.0302 - val_loss: 1.0792 - val_f1_score: 0.0217 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12826/12826 [==============================] - 45s 4ms/step - loss: 0.9273 - f1_score: 0.0302 - val_loss: 1.0418 - val_f1_score: 0.0214 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12826/12826 [==============================] - 51s 4ms/step - loss: 0.9164 - f1_score: 0.0302 - val_loss: 0.9633 - val_f1_score: 0.0226 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12826/12826 [==============================] - 53s 4ms/step - loss: 0.9078 - f1_score: 0.0303 - val_loss: 0.9064 - val_f1_score: 0.0235 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 0.8991 - f1_score: 0.0303 - val_loss: 0.8650 - val_f1_score: 0.0246 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12826/12826 [==============================] - 47s 4ms/step - loss: 0.8926 - f1_score: 0.0303 - val_loss: 0.9501 - val_f1_score: 0.0232 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12826/12826 [==============================] - 53s 4ms/step - loss: 0.8850 - f1_score: 0.0303 - val_loss: 0.8195 - val_f1_score: 0.0253 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12826/12826 [==============================] - 49s 4ms/step - loss: 0.8794 - f1_score: 0.0304 - val_loss: 0.7950 - val_f1_score: 0.0252 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12826/12826 [==============================] - 31s 2ms/step - loss: 0.8729 - f1_score: 0.0303 - val_loss: 0.7501 - val_f1_score: 0.0263 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12826/12826 [==============================] - 42s 3ms/step - loss: 0.8654 - f1_score: 0.0304 - val_loss: 0.7844 - val_f1_score: 0.0258 - lr: 9.0484e-04\n",
      "Epoch 12/100\n",
      "12826/12826 [==============================] - 54s 4ms/step - loss: 0.8595 - f1_score: 0.0304 - val_loss: 0.7745 - val_f1_score: 0.0261 - lr: 8.1873e-04\n",
      "Epoch 13/100\n",
      "12826/12826 [==============================] - 45s 4ms/step - loss: 0.8527 - f1_score: 0.0304 - val_loss: 0.7222 - val_f1_score: 0.0264 - lr: 7.4082e-04\n",
      "Epoch 14/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 0.8481 - f1_score: 0.0304 - val_loss: 0.6945 - val_f1_score: 0.0267 - lr: 6.7032e-04\n",
      "Epoch 15/100\n",
      "12826/12826 [==============================] - 46s 4ms/step - loss: 0.8421 - f1_score: 0.0304 - val_loss: 0.6013 - val_f1_score: 0.0279 - lr: 6.0653e-04\n",
      "Epoch 16/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 0.8382 - f1_score: 0.0304 - val_loss: 0.5693 - val_f1_score: 0.0280 - lr: 5.4881e-04\n",
      "Epoch 17/100\n",
      "12826/12826 [==============================] - 40s 3ms/step - loss: 0.8333 - f1_score: 0.0304 - val_loss: 0.6224 - val_f1_score: 0.0273 - lr: 4.9659e-04\n",
      "Epoch 18/100\n",
      "12826/12826 [==============================] - 40s 3ms/step - loss: 0.8308 - f1_score: 0.0304 - val_loss: 0.6075 - val_f1_score: 0.0276 - lr: 4.4933e-04\n",
      "Epoch 19/100\n",
      "12826/12826 [==============================] - 38s 3ms/step - loss: 0.8279 - f1_score: 0.0304 - val_loss: 0.6174 - val_f1_score: 0.0274 - lr: 4.0657e-04\n",
      "Perte finale sur le jeu d'entraînement: 0.8279057145118713\n",
      "Exactitude finale sur le jeu d'entraînement: 0.030440738424658775\n",
      "Perte finale sur le jeu de validation: 0.617357075214386\n",
      "Exactitude finale sur le jeu de validation: 0.027393579483032227\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 4 -  métrique f1-score avec classifivation multiclasses \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 200 - 100 - 50 - 30 - 4\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 4.1 - CONSTRUCTION DU MODELE (Classification multiclasse - SMOTE - f1-Score)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"tanh\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"tanh\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 4.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Définition du F1-score multiclasses\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Convertir en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=tf.shape(y_pred)[-1])\n",
    "\n",
    "    # Calcul des true positives, predicted positives et possible positives\n",
    "    true_positives = tf.reduce_sum(tf.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = tf.reduce_sum(tf.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)  # Remplacer les NaN par des zéros\n",
    "\n",
    "    return tf.reduce_mean(f1)  # Moyenne sur toutes les classes\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score])\n",
    "\n",
    "# TEST 4.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test4 = model.fit(\n",
    "    X_train_multimodal_resampled, \n",
    "    y_train_multimodal_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test4.history['loss']\n",
    "train_accuracy_history = history_test4.history['f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test4.history.get('val_loss')\n",
    "val_accuracy_history = history_test4.history.get('val_f1_score')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "794f7d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 14s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77    114001\n",
      "           1       0.71      0.51      0.59    108339\n",
      "           2       0.42      0.61      0.50     41182\n",
      "           3       0.32      0.68      0.43      7195\n",
      "\n",
      "    accuracy                           0.65    270717\n",
      "   macro avg       0.55      0.65      0.57    270717\n",
      "weighted avg       0.68      0.65      0.65    270717\n",
      "\n",
      "[[89351 15144  8117  1389]\n",
      " [24219 55323 25060  3737]\n",
      " [ 3168  7609 25151  5254]\n",
      " [  205   330  1788  4872]]\n",
      "2822/2822 [==============================] - 5s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.76      0.78      0.77     38039\n",
      "           1       0.69      0.50      0.58     35848\n",
      "           2       0.39      0.56      0.46     13946\n",
      "           3       0.17      0.36      0.23      2354\n",
      "\n",
      "    accuracy                           0.62     90273\n",
      "   macro avg       0.40      0.44      0.41     90273\n",
      "weighted avg       0.66      0.62      0.63     90273\n",
      "\n",
      "[[    0    83     2     1     0]\n",
      " [    0 29592  5068  2767   612]\n",
      " [    0  8180 17826  8355  1487]\n",
      " [    0  1149  2825  7852  2120]\n",
      " [    0   126   290  1090   848]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 4.4 - EVALUATION DU MODELE (multiclasse - f1-Score)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_multimodal, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_multimodal,y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bab596",
   "metadata": {},
   "source": [
    "<u>TEST 5 - CLASSIFICATION MULTICLASSE - METRIQUE=F1-SCORE PONDERE - LOSS=SPARSE_CATEGORIAL_ENTROPY - SMOTE<u> \n",
    "Augmentation du nb de neurones par couche\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d12b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,954\n",
      "Trainable params: 60,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12826/12826 [==============================] - 76s 6ms/step - loss: 0.9798 - named_weighted_f1_score: 7.6726 - val_loss: 1.0778 - val_named_weighted_f1_score: 5.2925 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12826/12826 [==============================] - 52s 4ms/step - loss: 0.9410 - named_weighted_f1_score: 7.7165 - val_loss: 1.0400 - val_named_weighted_f1_score: 5.8577 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12826/12826 [==============================] - 42s 3ms/step - loss: 0.9272 - named_weighted_f1_score: 7.7315 - val_loss: 1.1617 - val_named_weighted_f1_score: 4.8977 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12826/12826 [==============================] - 41s 3ms/step - loss: 0.9165 - named_weighted_f1_score: 7.7421 - val_loss: 0.9947 - val_named_weighted_f1_score: 5.7891 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12826/12826 [==============================] - 49s 4ms/step - loss: 0.9068 - named_weighted_f1_score: 7.7430 - val_loss: 0.8710 - val_named_weighted_f1_score: 6.3170 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12826/12826 [==============================] - 56s 4ms/step - loss: 0.8982 - named_weighted_f1_score: 7.7505 - val_loss: 0.7550 - val_named_weighted_f1_score: 6.7008 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12826/12826 [==============================] - 44s 3ms/step - loss: 0.8904 - named_weighted_f1_score: 7.7589 - val_loss: 0.8889 - val_named_weighted_f1_score: 6.1549 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12826/12826 [==============================] - 53s 4ms/step - loss: 0.8827 - named_weighted_f1_score: 7.7635 - val_loss: 0.8127 - val_named_weighted_f1_score: 6.4467 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12826/12826 [==============================] - 51s 4ms/step - loss: 0.8763 - named_weighted_f1_score: 7.7646 - val_loss: 0.8302 - val_named_weighted_f1_score: 6.4040 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 0.8762771487236023\n",
      "Exactitude finale sur le jeu d'entraînement: 7.764566898345947\n",
      "Perte finale sur le jeu de validation: 0.8302352428436279\n",
      "Exactitude finale sur le jeu de validation: 6.403956413269043\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 5 -  métrique f1-score pondéré avec classifivation multiclasses \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 200 - 100 - 50 - 30 - 4\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 5.1 - CONSTRUCTION DU MODELE (Classification multiclasse - SMOTE - f1-Score pondéré)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"tanh\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"tanh\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 5.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Utilisation d'une factory de fonction afin de permettre le passage en paramètre de class_weights\n",
    "\n",
    "def weighted_f1_score(y_true, y_pred, class_weights):\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "\n",
    "    # Convertir les labels en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=len(class_weights))\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=len(class_weights))\n",
    "\n",
    "    # Précision et rappel pour chaque classe\n",
    "    true_positives = K.sum(K.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = K.sum(K.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = K.sum(K.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    # F1-score par classe\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    # Conversion des poids en float32\n",
    "    class_weights = K.cast(class_weights, dtype='float32')\n",
    "\n",
    "    # Calcul du f1-score pondéré\n",
    "    weighted_f1 = K.sum(f1_scores * class_weights) / K.sum(class_weights)\n",
    "\n",
    "    return weighted_f1\n",
    "\n",
    "\n",
    "# Calcul des poids des classes\n",
    "unique_classes, class_counts = np.unique(y_train_multimodal_resampled, return_counts=True)\n",
    "class_weights = class_counts / len(y_train_multimodal_resampled)\n",
    "class_weights = 1.0 / class_weights\n",
    "\n",
    "# Création de la fonction weighted à partir de la factory afin de passer 2 des arguments lors de l'appel dans le compile, le 3ème (class_weights étant prés transmis via la factory\n",
    "def make_weighted_f1_score(class_weights):\n",
    "    def named_weighted_f1_score(y_true, y_pred):\n",
    "        return weighted_f1_score(y_true, y_pred, class_weights)\n",
    "    return named_weighted_f1_score\n",
    "\n",
    "weighted_f1 = make_weighted_f1_score(class_weights)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              #metrics=[lambda y_true, y_pred: weighted_f1_score(y_true, y_pred, class_weights)])\n",
    "              metrics=[weighted_f1])\n",
    "\n",
    "# TEST 5.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test5 = model.fit(\n",
    "    X_train_multimodal_resampled, \n",
    "    y_train_multimodal_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test5.history['loss']\n",
    "train_accuracy_history = history_test5.history['named_weighted_f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test5.history.get('val_loss')\n",
    "val_accuracy_history = history_test5.history.get('val_named_weighted_f1_score')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7aa997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 21s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77    114001\n",
      "           1       0.69      0.50      0.58    108339\n",
      "           2       0.39      0.54      0.45     41182\n",
      "           3       0.21      0.63      0.31      7195\n",
      "\n",
      "    accuracy                           0.62    270717\n",
      "   macro avg       0.51      0.61      0.53    270717\n",
      "weighted avg       0.66      0.62      0.63    270717\n",
      "\n",
      "[[87279 16697  7625  2400]\n",
      " [23077 53729 25308  6225]\n",
      " [ 3265  7137 22333  8447]\n",
      " [  269   481  1926  4519]]\n",
      "2822/2822 [==============================] - 8s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.76      0.76      0.76     38039\n",
      "           1       0.67      0.49      0.57     35848\n",
      "           2       0.38      0.52      0.44     13946\n",
      "           3       0.15      0.46      0.23      2354\n",
      "\n",
      "    accuracy                           0.61     90273\n",
      "   macro avg       0.39      0.45      0.40     90273\n",
      "weighted avg       0.65      0.61      0.62     90273\n",
      "\n",
      "[[    0    81     1     3     1]\n",
      " [    0 28947  5647  2511   934]\n",
      " [    0  7728 17471  8500  2149]\n",
      " [    0  1093  2581  7213  3059]\n",
      " [    0   113   271   877  1093]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 5.4 - EVALUATION DU MODELE (multiclasse - f1-Score pondéré)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_multimodal, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_multimodal,y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbb3b5",
   "metadata": {},
   "source": [
    "<u>TEST 6 - CLASSIFICATION MULTICLASSE - METRIQUE=F1-SCORE - LOSS=SPARSE_CATEGORIAL_ENTROPY - SMOTE<u> \n",
    "\n",
    "Augmentation du nb de neurones par couche et remplacement fonction d'activation tanh par ReLu pour faire ressortir des relations non linéaires simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39417386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,954\n",
      "Trainable params: 60,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12826/12826 [==============================] - 51s 4ms/step - loss: 0.9217 - f1_score: 0.0301 - val_loss: 0.8714 - val_f1_score: 0.0238 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12826/12826 [==============================] - 37s 3ms/step - loss: 0.8392 - f1_score: 0.0303 - val_loss: 0.6661 - val_f1_score: 0.0269 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 0.8045 - f1_score: 0.0304 - val_loss: 0.6028 - val_f1_score: 0.0272 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 0.7846 - f1_score: 0.0304 - val_loss: 0.5311 - val_f1_score: 0.0279 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12826/12826 [==============================] - 40s 3ms/step - loss: 0.7682 - f1_score: 0.0304 - val_loss: 0.5476 - val_f1_score: 0.0276 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12826/12826 [==============================] - 45s 4ms/step - loss: 0.7564 - f1_score: 0.0305 - val_loss: 0.5315 - val_f1_score: 0.0277 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12826/12826 [==============================] - 38s 3ms/step - loss: 0.7470 - f1_score: 0.0305 - val_loss: 0.4056 - val_f1_score: 0.0289 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12826/12826 [==============================] - 40s 3ms/step - loss: 0.7401 - f1_score: 0.0305 - val_loss: 0.3673 - val_f1_score: 0.0291 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12826/12826 [==============================] - 38s 3ms/step - loss: 0.7337 - f1_score: 0.0305 - val_loss: 0.4537 - val_f1_score: 0.0284 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12826/12826 [==============================] - 38s 3ms/step - loss: 0.7274 - f1_score: 0.0305 - val_loss: 0.3847 - val_f1_score: 0.0290 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12826/12826 [==============================] - 38s 3ms/step - loss: 0.7181 - f1_score: 0.0305 - val_loss: 0.4646 - val_f1_score: 0.0282 - lr: 9.0484e-04\n",
      "Perte finale sur le jeu d'entraînement: 0.7180730700492859\n",
      "Exactitude finale sur le jeu d'entraînement: 0.030525049194693565\n",
      "Perte finale sur le jeu de validation: 0.46462562680244446\n",
      "Exactitude finale sur le jeu de validation: 0.028204485774040222\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 6 -  métrique f1-score avec classifivation multiclasses \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 200 - 100 - 50 - 30 - 4\n",
    "#          FONCTION D'ACTIVATION RELU au lieu de tanh\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 6.1 - CONSTRUCTION DU MODELE (Classification multiclasse - SMOTE - f1-Score)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"relu\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"relu\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"relu\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 6.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Définition du F1-score multiclasses\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Convertir en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=tf.shape(y_pred)[-1])\n",
    "\n",
    "    # Calcul des true positives, predicted positives et possible positives\n",
    "    true_positives = tf.reduce_sum(tf.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = tf.reduce_sum(tf.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)  # Remplacer les NaN par des zéros\n",
    "\n",
    "    return tf.reduce_mean(f1)  # Moyenne sur toutes les classes\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score])\n",
    "\n",
    "# TEST 6.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test6 = model.fit(\n",
    "    X_train_multimodal_resampled, \n",
    "    y_train_multimodal_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test6.history['loss']\n",
    "train_accuracy_history = history_test6.history['f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test6.history.get('val_loss')\n",
    "val_accuracy_history = history_test6.history.get('val_f1_score')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9af7c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 12s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.79      0.78    114001\n",
      "           1       0.70      0.56      0.62    108339\n",
      "           2       0.46      0.65      0.54     41182\n",
      "           3       0.51      0.60      0.55      7195\n",
      "\n",
      "    accuracy                           0.67    270717\n",
      "   macro avg       0.61      0.65      0.62    270717\n",
      "weighted avg       0.69      0.67      0.67    270717\n",
      "\n",
      "[[90350 15795  7190   666]\n",
      " [24431 60183 22234  1491]\n",
      " [ 3321  9205 26754  1902]\n",
      " [  159   386  2353  4297]]\n",
      "2822/2822 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.75      0.78      0.77     38039\n",
      "           1       0.67      0.53      0.59     35848\n",
      "           2       0.40      0.57      0.47     13946\n",
      "           3       0.18      0.20      0.19      2354\n",
      "\n",
      "    accuracy                           0.63     90273\n",
      "   macro avg       0.40      0.42      0.40     90273\n",
      "weighted avg       0.65      0.63      0.64     90273\n",
      "\n",
      "[[    0    83     2     1     0]\n",
      " [    0 29643  5419  2669   308]\n",
      " [    0  8327 19093  7718   710]\n",
      " [    0  1267  3688  7960  1031]\n",
      " [    0   138   434  1321   461]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 6.4 - EVALUATION DU MODELE (multiclasse - f1-Score)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_multimodal, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_multimodal,y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390fcfa",
   "metadata": {},
   "source": [
    "<u>TEST 7 - CLASSIFICATION MULTICLASSE - METRIQUE=F1-SCORE - LOSS=SPARSE_CATEGORIAL_ENTROPY - SMOTE<u> \n",
    "\n",
    "Augmentation du nb de neurones par couche et remplacement fonction d'activation tanh par une fonction polynomiale cubic pour faire ressortir des relations non linéaires plus complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d7f5bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 4)                 204       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,954\n",
      "Trainable params: 60,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12826/12826 [==============================] - 47s 3ms/step - loss: 1.1418 - f1_score: 0.0270 - val_loss: 1.0679 - val_f1_score: 0.0202 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12826/12826 [==============================] - 40s 3ms/step - loss: 81.4260 - f1_score: 0.0296 - val_loss: 1.1107 - val_f1_score: 0.0189 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12826/12826 [==============================] - 37s 3ms/step - loss: 427.9957 - f1_score: 0.0298 - val_loss: 0.9933 - val_f1_score: 0.0236 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12826/12826 [==============================] - 35s 3ms/step - loss: 2.1128 - f1_score: 0.0299 - val_loss: 0.9460 - val_f1_score: 0.0238 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12826/12826 [==============================] - 34s 3ms/step - loss: 0.9941 - f1_score: 0.0298 - val_loss: 1.0841 - val_f1_score: 0.0200 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12826/12826 [==============================] - 39s 3ms/step - loss: 1.0458 - f1_score: 0.0299 - val_loss: 0.9127 - val_f1_score: 0.0258 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12826/12826 [==============================] - 33s 3ms/step - loss: 0.9801 - f1_score: 0.0300 - val_loss: 0.7972 - val_f1_score: 0.0266 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12826/12826 [==============================] - 37s 3ms/step - loss: 0.9618 - f1_score: 0.0300 - val_loss: 0.7571 - val_f1_score: 0.0270 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12826/12826 [==============================] - 40s 3ms/step - loss: 0.9885 - f1_score: 0.0300 - val_loss: 0.7199 - val_f1_score: 0.0281 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12826/12826 [==============================] - 29s 2ms/step - loss: 0.9619 - f1_score: 0.0301 - val_loss: 0.6555 - val_f1_score: 0.0284 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12826/12826 [==============================] - 29s 2ms/step - loss: 0.9482 - f1_score: 0.0301 - val_loss: 0.6746 - val_f1_score: 0.0285 - lr: 9.0484e-04\n",
      "Epoch 12/100\n",
      "12826/12826 [==============================] - 30s 2ms/step - loss: 0.9094 - f1_score: 0.0302 - val_loss: 0.6980 - val_f1_score: 0.0276 - lr: 8.1873e-04\n",
      "Epoch 13/100\n",
      "12826/12826 [==============================] - 29s 2ms/step - loss: 0.8919 - f1_score: 0.0302 - val_loss: 0.4555 - val_f1_score: 0.0299 - lr: 7.4082e-04\n",
      "Epoch 14/100\n",
      "12826/12826 [==============================] - 30s 2ms/step - loss: 0.8721 - f1_score: 0.0303 - val_loss: 0.6032 - val_f1_score: 0.0283 - lr: 6.7032e-04\n",
      "Epoch 15/100\n",
      "12826/12826 [==============================] - 36s 3ms/step - loss: 0.8690 - f1_score: 0.0302 - val_loss: 0.7185 - val_f1_score: 0.0270 - lr: 6.0653e-04\n",
      "Epoch 16/100\n",
      "12826/12826 [==============================] - 30s 2ms/step - loss: 0.8568 - f1_score: 0.0303 - val_loss: 0.5420 - val_f1_score: 0.0286 - lr: 5.4881e-04\n",
      "Perte finale sur le jeu d'entraînement: 0.8568196892738342\n",
      "Exactitude finale sur le jeu d'entraînement: 0.030274001881480217\n",
      "Perte finale sur le jeu de validation: 0.5419865846633911\n",
      "Exactitude finale sur le jeu de validation: 0.028634509071707726\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 7 -  métrique f1-score avec classifivation multiclasses \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 200 - 100 - 50 - 30 - 4\n",
    "#          FONCTION D'ACTIVATION POLYNOMIALE CUBIC au lieu de tanh\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 7.1 - CONSTRUCTION DU MODELE (Classification multiclasse - SMOTE - f1-Score)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def polynomial_cubic(x):\n",
    "    return tf.math.pow(x, 3)\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = polynomial_cubic, name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = polynomial_cubic, name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = polynomial_cubic, name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 4, activation = \"softmax\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 7.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Définition du F1-score multiclasses\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_true = tf.cast(y_true, tf.int32)\n",
    "\n",
    "    # Convertir en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=tf.shape(y_pred)[-1])\n",
    "\n",
    "    # Calcul des true positives, predicted positives et possible positives\n",
    "    true_positives = tf.reduce_sum(tf.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = tf.reduce_sum(tf.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)  # Remplacer les NaN par des zéros\n",
    "\n",
    "    return tf.reduce_mean(f1)  # Moyenne sur toutes les classes\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score])\n",
    "\n",
    "# TEST 7.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test7 = model.fit(\n",
    "    X_train_multimodal_resampled, \n",
    "    y_train_multimodal_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test7.history['loss']\n",
    "train_accuracy_history = history_test7.history['f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test7.history.get('val_loss')\n",
    "val_accuracy_history = history_test7.history.get('val_f1_score')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae467368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 13s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.77    114001\n",
      "           1       0.70      0.44      0.54    108339\n",
      "           2       0.38      0.51      0.44     41182\n",
      "           3       0.21      0.73      0.33      7195\n",
      "\n",
      "    accuracy                           0.61    270717\n",
      "   macro avg       0.51      0.62      0.52    270717\n",
      "weighted avg       0.66      0.61      0.61    270717\n",
      "\n",
      "[[90462 12770  8203  2566]\n",
      " [28138 47689 25268  7244]\n",
      " [ 3251  7017 21156  9758]\n",
      " [  191   297  1441  5266]]\n",
      "2822/2822 [==============================] - 4s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        86\n",
      "           0       0.74      0.79      0.76     38039\n",
      "           1       0.68      0.43      0.53     35848\n",
      "           2       0.35      0.48      0.41     13946\n",
      "           3       0.13      0.46      0.20      2354\n",
      "\n",
      "    accuracy                           0.59     90273\n",
      "   macro avg       0.38      0.43      0.38     90273\n",
      "weighted avg       0.64      0.59      0.60     90273\n",
      "\n",
      "[[    0    79     4     2     1]\n",
      " [    0 29974  4178  2822  1065]\n",
      " [    0  9370 15403  8372  2703]\n",
      " [    0  1218  2643  6647  3438]\n",
      " [    0   113   270   899  1072]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 7.4 - EVALUATION DU MODELE (multiclasse - f1-Score)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = np.argmax(y_train_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_multimodal, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_multimodal,y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_multimodal, y_pred_class))\n",
    "print(confusion_matrix(y_test_multimodal,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c78df",
   "metadata": {},
   "source": [
    "<u>TEST 8 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE - LOSS=BINARY_CROSSENTROPY - SMOTE</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3309ced9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1780      \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_76 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,909\n",
      "Trainable params: 1,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 61s 4ms/step - loss: 0.4910 - f1_score_metric: 0.7372 - val_loss: 0.5464 - val_f1_score_metric: 0.8605 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.4742 - f1_score_metric: 0.7429 - val_loss: 0.5064 - val_f1_score_metric: 0.8754 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 40s 3ms/step - loss: 0.4708 - f1_score_metric: 0.7392 - val_loss: 0.5201 - val_f1_score_metric: 0.8695 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 22s 2ms/step - loss: 0.4695 - f1_score_metric: 0.7399 - val_loss: 0.4993 - val_f1_score_metric: 0.8628 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 23s 2ms/step - loss: 0.4691 - f1_score_metric: 0.7385 - val_loss: 0.4749 - val_f1_score_metric: 0.8597 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12507/12507 [==============================] - 24s 2ms/step - loss: 0.4685 - f1_score_metric: 0.7379 - val_loss: 0.4632 - val_f1_score_metric: 0.8769 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12507/12507 [==============================] - 22s 2ms/step - loss: 0.4681 - f1_score_metric: 0.7393 - val_loss: 0.4822 - val_f1_score_metric: 0.8550 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12507/12507 [==============================] - 33s 3ms/step - loss: 0.4681 - f1_score_metric: 0.7362 - val_loss: 0.5348 - val_f1_score_metric: 0.8247 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12507/12507 [==============================] - 31s 3ms/step - loss: 0.4678 - f1_score_metric: 0.7370 - val_loss: 0.4750 - val_f1_score_metric: 0.8538 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 0.4677775800228119\n",
      "Exactitude finale sur le jeu d'entraînement: 0.7370425462722778\n",
      "Perte finale sur le jeu de validation: 0.4750359356403351\n",
      "Exactitude finale sur le jeu de validation: 0.8538069128990173\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 8 -  métrique f1-score avec classifivation binaire (loss=binary_crossentropy) + SMOTE\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Le f1-score est une métrique plutôt utilisé pour des problèmes de classification binaire\n",
    "# la métrique f1-score ne semble pas prise en charge par la fonction \"compile\" de keras \n",
    "# --> Obligation de passer par une fonction d'évaluation personnalisée :  ici f1_score_metric\n",
    "\n",
    "# 1. Construction d'un DNN sur la base d'une structure similaire à celle du cours (Difficile de définir la structure optimale qui répondra le mieux à notre problème : on réutilise le travail des autres)\n",
    "# Réseau configuré pour de la classification multimodales (maintien des 4 classes cibles)\n",
    "# Ajout de couche dropout après chaque couche Dense afin d'éviter l'overfitting\n",
    "\n",
    "# TEST 8.1 - Construction du modèle\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.4)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.4)\n",
    "\n",
    "dense3 = Dense(units = 4, activation = \"tanh\", name = \"Dense_3\")\n",
    "Dropout3 = Dropout(0.4)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 8.2 - COMPILATION DU MODELE (classification binaire - SMOTE - f1-score)\n",
    "# --------------------------------\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "# A privilégier pour la classification binaire\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score_metric])\n",
    "\n",
    "\n",
    "# TEST 8.3 - APPRENTISSAGE DU MODELE - classification binaire - f1-Score - SMOTE\n",
    "# ----------------------------------\n",
    "history_test8 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test8.history['loss']\n",
    "train_accuracy_history = history_test8.history['f1_score_metric']\n",
    "\n",
    "# Récupération des scores de validation\n",
    "val_loss_history = history_test8.history.get('val_loss')\n",
    "val_accuracy_history = history_test8.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n",
    "# Sauvegarde du modèle (il sera utilisé pour la partie interprétabilité)\n",
    "model.save(repModels + 'DNNmodel_10_8_4.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "310fda93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 12s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85    222340\n",
      "           1       0.43      0.76      0.55     48377\n",
      "\n",
      "    accuracy                           0.78    270717\n",
      "   macro avg       0.68      0.77      0.70    270717\n",
      "weighted avg       0.85      0.78      0.80    270717\n",
      "\n",
      "[[174080  48260]\n",
      " [ 11766  36611]]\n",
      "2822/2822 [==============================] - 5s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85     73973\n",
      "           1       0.43      0.75      0.55     16300\n",
      "\n",
      "    accuracy                           0.78     90273\n",
      "   macro avg       0.68      0.77      0.70     90273\n",
      "weighted avg       0.84      0.78      0.80     90273\n",
      "\n",
      "[[57890 16083]\n",
      " [ 4018 12282]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 8.4 - EVALUATION DU MODELE - classification binaire - f1-Score - SMOTE\n",
    "# -----------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# EVALUATION DU MODELE sur échantillon train\n",
    "y_train_pred=model.predict(X_train)\n",
    "threshold = 0.5\n",
    "y_train_pred_class = (y_train_pred >= threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "\n",
    "# EVALUATION DU MODELE sur échantillon test\n",
    "y_pred=model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_pred_class = (y_pred >= threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e505336",
   "metadata": {},
   "source": [
    "<u>TEST 9 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE pondéré - LOSS=BINARY_CROSSENTROPY - SMOTE</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88c67ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1780      \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,909\n",
      "Trainable params: 1,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 49s 3ms/step - loss: 0.4910 - f1_score_metric: 0.7341 - val_loss: 0.4678 - val_f1_score_metric: 0.8530 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 30s 2ms/step - loss: 0.4707 - f1_score_metric: 0.7523 - val_loss: 0.4227 - val_f1_score_metric: 0.8849 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.4669 - f1_score_metric: 0.7540 - val_loss: 0.4200 - val_f1_score_metric: 0.8753 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 29s 2ms/step - loss: 0.4649 - f1_score_metric: 0.7560 - val_loss: 0.3988 - val_f1_score_metric: 0.8898 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 35s 3ms/step - loss: 0.4634 - f1_score_metric: 0.7556 - val_loss: 0.3687 - val_f1_score_metric: 0.8949 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12507/12507 [==============================] - 35s 3ms/step - loss: 0.4626 - f1_score_metric: 0.7564 - val_loss: 0.3937 - val_f1_score_metric: 0.8898 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.4618 - f1_score_metric: 0.7601 - val_loss: 0.4071 - val_f1_score_metric: 0.8876 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12507/12507 [==============================] - 30s 2ms/step - loss: 0.4616 - f1_score_metric: 0.7611 - val_loss: 0.3703 - val_f1_score_metric: 0.8985 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 0.4616474509239197\n",
      "Exactitude finale sur le jeu d'entraînement: 0.7610890865325928\n",
      "Perte finale sur le jeu de validation: 0.37029555439949036\n",
      "Exactitude finale sur le jeu de validation: 0.8985297083854675\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 9 -  métrique f1-score avec classifivation binaire (loss=binary_crossentropy) + SMOTE\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Remplacement fonction d'activation tanh par reLu pour rechercher des relation non linéaires simples\n",
    "# TEST 9.1 - Construction du modèle\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"relu\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.4)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"relu\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.4)\n",
    "\n",
    "dense3 = Dense(units = 4, activation = \"relu\", name = \"Dense_3\")\n",
    "Dropout3 = Dropout(0.4)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 9.2 - COMPILATION DU MODELE (classification binaire - SMOTE - f1-score)\n",
    "# --------------------------------\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "# A privilégier pour la classification binaire\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score_metric])\n",
    "\n",
    "\n",
    "# TEST 9.3 - APPRENTISSAGE DU MODELE - classification binaire - f1-Score - SMOTE\n",
    "# ----------------------------------\n",
    "history_test9 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test9.history['loss']\n",
    "train_accuracy_history = history_test9.history['f1_score_metric']\n",
    "\n",
    "# Récupération des scores de validation\n",
    "val_loss_history = history_test9.history.get('val_loss')\n",
    "val_accuracy_history = history_test9.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n",
    "# Sauvegarde du modèle (il sera utilisé pour la partie interprétabilité)\n",
    "model.save(repModels + 'DNNmodel_10_8_4_test9.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db2e979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 19s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86    222340\n",
      "           1       0.45      0.77      0.57     48377\n",
      "\n",
      "    accuracy                           0.79    270717\n",
      "   macro avg       0.69      0.78      0.71    270717\n",
      "weighted avg       0.85      0.79      0.81    270717\n",
      "\n",
      "[[175777  46563]\n",
      " [ 10918  37459]]\n",
      "2822/2822 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86     73973\n",
      "           1       0.44      0.77      0.56     16300\n",
      "\n",
      "    accuracy                           0.78     90273\n",
      "   macro avg       0.69      0.78      0.71     90273\n",
      "weighted avg       0.85      0.78      0.80     90273\n",
      "\n",
      "[[58225 15748]\n",
      " [ 3777 12523]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 9.4 - EVALUATION DU MODELE - classification binaire - f1-Score - SMOTE - activation=RELU\n",
    "# -----------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# EVALUATION DU MODELE sur échantillon train\n",
    "y_train_pred=model.predict(X_train)\n",
    "threshold = 0.5\n",
    "y_train_pred_class = (y_train_pred >= threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "\n",
    "# EVALUATION DU MODELE sur échantillon test\n",
    "y_pred=model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_pred_class = (y_pred >= threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb500f6",
   "metadata": {},
   "source": [
    "<u>TEST 10 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE pondéré - LOSS=BINARY_CROSSENTROPY - SMOTE</u>\n",
    "activation=tanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d358e49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1780      \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,909\n",
      "Trainable params: 1,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 40s 3ms/step - loss: 0.4752 - named_weighted_f1_score: 11.3615 - val_loss: 0.5230 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 24s 2ms/step - loss: 0.4620 - named_weighted_f1_score: 11.3627 - val_loss: 0.4294 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 23s 2ms/step - loss: 0.4590 - named_weighted_f1_score: 11.3620 - val_loss: 0.5018 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 33s 3ms/step - loss: 0.4548 - named_weighted_f1_score: 11.3628 - val_loss: 0.5039 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 34s 3ms/step - loss: 0.4535 - named_weighted_f1_score: 11.3613 - val_loss: 0.4957 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 0.45346158742904663\n",
      "Exactitude finale sur le jeu d'entraînement: 11.3612699508667\n",
      "Perte finale sur le jeu de validation: 0.49565058946609497\n",
      "Exactitude finale sur le jeu de validation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 10 -  métrique f1-score pondéré avec classifivation binaire \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 10 - 8 - 4 - 1\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 10.1 - CONSTRUCTION DU MODELE (Classification binaire - SMOTE - f1-Score pondéré)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"tanh\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 4, activation = \"tanh\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 10.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Utilisation d'une factory de fonction afin de permettre le passage en paramètre de class_weights\n",
    "def weighted_f1_score(y_true, y_pred, class_weights):\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "\n",
    "    # Convertir les labels en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=len(class_weights))\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=len(class_weights))\n",
    "\n",
    "    # Précision et rappel pour chaque classe\n",
    "    true_positives = K.sum(K.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = K.sum(K.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = K.sum(K.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    # F1-score par classe\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    # Conversion des poids en float32\n",
    "    class_weights = K.cast(class_weights, dtype='float32')\n",
    "\n",
    "    # Calcul du f1-score pondéré\n",
    "    weighted_f1 = K.sum(f1_scores * class_weights) / K.sum(class_weights)\n",
    "\n",
    "    return weighted_f1\n",
    "\n",
    "\n",
    "# Calcul des poids des classes\n",
    "unique_classes, class_counts = np.unique(y_train_binary_resampled, return_counts=True)\n",
    "class_weights = class_counts / len(y_train_binary_resampled)\n",
    "class_weights = 1.0 / class_weights\n",
    "\n",
    "# Création de la fonction weighted à partir de la factory afin de passer 2 des arguments lors de l'appel dans le compile, le 3ème (class_weights étant prés transmis via la factory\n",
    "def make_weighted_f1_score(class_weights):\n",
    "    def named_weighted_f1_score(y_true, y_pred):\n",
    "        return weighted_f1_score(y_true, y_pred, class_weights)\n",
    "    return named_weighted_f1_score\n",
    "\n",
    "weighted_f1 = make_weighted_f1_score(class_weights)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[weighted_f1])\n",
    "              #metrics=[f1_score_metric])  \n",
    "\n",
    "# TEST 10.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test10 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test10.history['loss']\n",
    "train_accuracy_history = history_test10.history['named_weighted_f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "#train_accuracy_history = history_test5.history['f1_score_metric']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test10.history.get('val_loss')\n",
    "val_accuracy_history = history_test10.history.get('val_named_weighted_f1_score')\n",
    "#val_accuracy_history = history_test5.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49871d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 18s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84    222340\n",
      "           1       0.41      0.79      0.54     48377\n",
      "\n",
      "    accuracy                           0.76    270717\n",
      "   macro avg       0.68      0.77      0.69    270717\n",
      "weighted avg       0.85      0.76      0.79    270717\n",
      "\n",
      "[[167970  54370]\n",
      " [ 10116  38261]]\n",
      "2822/2822 [==============================] - 6s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.75      0.84     73973\n",
      "           1       0.42      0.79      0.54     16300\n",
      "\n",
      "    accuracy                           0.76     90273\n",
      "   macro avg       0.68      0.77      0.69     90273\n",
      "weighted avg       0.85      0.76      0.79     90273\n",
      "\n",
      "[[55835 18138]\n",
      " [ 3413 12887]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 10.4 - EVALUATION DU MODELE (binary - f1-Score pondéré - activation=tanh)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = (y_train_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n",
    "# 0.77 - 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f44ef",
   "metadata": {},
   "source": [
    "<u>TEST 11 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE pondéré - LOSS=BINARY_CROSSENTROPY - SMOTE</u>\n",
    "activation=relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a185e104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 10)                1780      \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 8)                 88        \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 8)                 0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,909\n",
      "Trainable params: 1,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 39s 3ms/step - loss: 0.4777 - named_weighted_f1_score: 11.3624 - val_loss: 0.4951 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 21s 2ms/step - loss: 0.4563 - named_weighted_f1_score: 11.3627 - val_loss: 0.4895 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 21s 2ms/step - loss: 0.4526 - named_weighted_f1_score: 11.3622 - val_loss: 0.5055 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 23s 2ms/step - loss: 0.4502 - named_weighted_f1_score: 11.3623 - val_loss: 0.4534 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 37s 3ms/step - loss: 0.4494 - named_weighted_f1_score: 11.3630 - val_loss: 0.5322 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12507/12507 [==============================] - 43s 3ms/step - loss: 0.4482 - named_weighted_f1_score: 11.3619 - val_loss: 0.5060 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12507/12507 [==============================] - 45s 4ms/step - loss: 0.4469 - named_weighted_f1_score: 11.3614 - val_loss: 0.4739 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 0.44688642024993896\n",
      "Exactitude finale sur le jeu d'entraînement: 11.361357688903809\n",
      "Perte finale sur le jeu de validation: 0.47394484281539917\n",
      "Exactitude finale sur le jeu de validation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 11 -  métrique f1-score pondéré avec classifivation binaire \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 10 - 8 - 4 - 1\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 10.1 - CONSTRUCTION DU MODELE (Classification binaire - SMOTE - f1-Score pondéré)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 10, activation = \"relu\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 8, activation = \"relu\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 4, activation = \"relu\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 11.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Utilisation d'une factory de fonction afin de permettre le passage en paramètre de class_weights\n",
    "def weighted_f1_score(y_true, y_pred, class_weights):\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "\n",
    "    # Convertir les labels en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=len(class_weights))\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=len(class_weights))\n",
    "\n",
    "    # Précision et rappel pour chaque classe\n",
    "    true_positives = K.sum(K.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = K.sum(K.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = K.sum(K.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    # F1-score par classe\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    # Conversion des poids en float32\n",
    "    class_weights = K.cast(class_weights, dtype='float32')\n",
    "\n",
    "    # Calcul du f1-score pondéré\n",
    "    weighted_f1 = K.sum(f1_scores * class_weights) / K.sum(class_weights)\n",
    "\n",
    "    return weighted_f1\n",
    "\n",
    "\n",
    "# Calcul des poids des classes\n",
    "unique_classes, class_counts = np.unique(y_train_binary_resampled, return_counts=True)\n",
    "class_weights = class_counts / len(y_train_binary_resampled)\n",
    "class_weights = 1.0 / class_weights\n",
    "\n",
    "# Création de la fonction weighted à partir de la factory afin de passer 2 des arguments lors de l'appel dans le compile, le 3ème (class_weights étant prés transmis via la factory\n",
    "def make_weighted_f1_score(class_weights):\n",
    "    def named_weighted_f1_score(y_true, y_pred):\n",
    "        return weighted_f1_score(y_true, y_pred, class_weights)\n",
    "    return named_weighted_f1_score\n",
    "\n",
    "weighted_f1 = make_weighted_f1_score(class_weights)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[weighted_f1])\n",
    "              #metrics=[f1_score_metric])  \n",
    "\n",
    "# TEST 11.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test11 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test11.history['loss']\n",
    "train_accuracy_history = history_test11.history['named_weighted_f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "#train_accuracy_history = history_test5.history['f1_score_metric']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test11.history.get('val_loss')\n",
    "val_accuracy_history = history_test11.history.get('val_named_weighted_f1_score')\n",
    "#val_accuracy_history = history_test5.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2dccf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 13s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86    222340\n",
      "           1       0.45      0.78      0.57     48377\n",
      "\n",
      "    accuracy                           0.79    270717\n",
      "   macro avg       0.69      0.79      0.71    270717\n",
      "weighted avg       0.85      0.79      0.81    270717\n",
      "\n",
      "[[175394  46946]\n",
      " [ 10547  37830]]\n",
      "2822/2822 [==============================] - 4s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86     73973\n",
      "           1       0.44      0.77      0.56     16300\n",
      "\n",
      "    accuracy                           0.78     90273\n",
      "   macro avg       0.69      0.78      0.71     90273\n",
      "weighted avg       0.85      0.78      0.80     90273\n",
      "\n",
      "[[58177 15796]\n",
      " [ 3695 12605]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 11.4 - EVALUATION DU MODELE (binary - f1-Score pondéré - activation=relu)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = (y_train_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n",
    "# 0.77 - 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49846e56",
   "metadata": {},
   "source": [
    "<u>TEST 12 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE - LOSS=BINARY_CROSSENTROPY - SMOTE</u>\n",
    "activation=tanh\n",
    "réseaux avec plus de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aac84ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,801\n",
      "Trainable params: 60,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 58s 4ms/step - loss: 0.4598 - f1_score_metric: 0.7509 - val_loss: 0.4586 - val_f1_score_metric: 0.8939 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 40s 3ms/step - loss: 0.4464 - f1_score_metric: 0.7603 - val_loss: 0.4771 - val_f1_score_metric: 0.8944 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 39s 3ms/step - loss: 0.4422 - f1_score_metric: 0.7643 - val_loss: 0.4267 - val_f1_score_metric: 0.9011 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.4391 - f1_score_metric: 0.7659 - val_loss: 0.4531 - val_f1_score_metric: 0.9026 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 36s 3ms/step - loss: 0.4364 - f1_score_metric: 0.7685 - val_loss: 0.4286 - val_f1_score_metric: 0.9076 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12507/12507 [==============================] - 41s 3ms/step - loss: 0.4342 - f1_score_metric: 0.7705 - val_loss: 0.4649 - val_f1_score_metric: 0.8888 - lr: 0.0010\n",
      "Perte finale sur le jeu d'entraînement: 0.434212327003479\n",
      "Exactitude finale sur le jeu d'entraînement: 0.7705172896385193\n",
      "Perte finale sur le jeu de validation: 0.4649337828159332\n",
      "Exactitude finale sur le jeu de validation: 0.8888211846351624\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 12 -  métrique f1-score avec classifivation binaire (loss=binary_crossentropy) + SMOTE\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Augmentation du nombre de neurones (200 - 100 - 50 - 1)\n",
    "\n",
    "# TEST 12.1 - Construction du modèle\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.4)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"tanh\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.4)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"tanh\", name = \"Dense_3\")\n",
    "Dropout3 = Dropout(0.4)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 12.2 - COMPILATION DU MODELE (classification binaire - SMOTE - f1-score)\n",
    "# --------------------------------\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "# A privilégier pour la classification binaire\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score_metric])\n",
    "\n",
    "\n",
    "# TEST 12.3 - APPRENTISSAGE DU MODELE - classification binaire - f1-Score - SMOTE\n",
    "# ----------------------------------\n",
    "history_test12 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test12.history['loss']\n",
    "train_accuracy_history = history_test12.history['f1_score_metric']\n",
    "\n",
    "# Récupération des scores de validation\n",
    "val_loss_history = history_test12.history.get('val_loss')\n",
    "val_accuracy_history = history_test12.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n",
    "# Sauvegarde du modèle (il sera utilisé pour la partie interprétabilité)\n",
    "model.save(repModels + 'DNNmodel_150_80_40.h5')\n",
    "#model.save(repModels + 'DNNmodel_10_8_6.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24049bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 13s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.78      0.85    222340\n",
      "           1       0.44      0.79      0.56     48377\n",
      "\n",
      "    accuracy                           0.78    270717\n",
      "   macro avg       0.69      0.79      0.71    270717\n",
      "weighted avg       0.85      0.78      0.80    270717\n",
      "\n",
      "[[173141  49199]\n",
      " [  9960  38417]]\n",
      "2822/2822 [==============================] - 8s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85     73973\n",
      "           1       0.44      0.79      0.56     16300\n",
      "\n",
      "    accuracy                           0.78     90273\n",
      "   macro avg       0.69      0.78      0.71     90273\n",
      "weighted avg       0.85      0.78      0.80     90273\n",
      "\n",
      "[[57535 16438]\n",
      " [ 3428 12872]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 12.4 - EVALUATION DU MODELE (binary - f1-Score - activation=tanh)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = (y_train_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n",
    "# 0.77 - 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7e890",
   "metadata": {},
   "source": [
    "<u>TEST 13 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE - LOSS=BINARY_CROSSENTROPY - SMOTE</u>\n",
    "activation=relu\n",
    "réseaux avec plus de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa60ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_62 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_63 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,801\n",
      "Trainable params: 60,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 54s 4ms/step - loss: 0.4398 - f1_score_metric: 0.7651 - val_loss: 0.4106 - val_f1_score_metric: 0.9102 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 46s 4ms/step - loss: 0.4085 - f1_score_metric: 0.7862 - val_loss: 0.3615 - val_f1_score_metric: 0.9307 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 55s 4ms/step - loss: 0.3941 - f1_score_metric: 0.7962 - val_loss: 0.3182 - val_f1_score_metric: 0.9364 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 53s 4ms/step - loss: 0.3830 - f1_score_metric: 0.8031 - val_loss: 0.3216 - val_f1_score_metric: 0.9349 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 38s 3ms/step - loss: 0.3736 - f1_score_metric: 0.8097 - val_loss: 0.3348 - val_f1_score_metric: 0.9374 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12507/12507 [==============================] - 28s 2ms/step - loss: 0.3673 - f1_score_metric: 0.8129 - val_loss: 0.2826 - val_f1_score_metric: 0.9432 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12507/12507 [==============================] - 29s 2ms/step - loss: 0.3628 - f1_score_metric: 0.8168 - val_loss: 0.2980 - val_f1_score_metric: 0.9462 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.3578 - f1_score_metric: 0.8187 - val_loss: 0.3173 - val_f1_score_metric: 0.9394 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.3525 - f1_score_metric: 0.8224 - val_loss: 0.2265 - val_f1_score_metric: 0.9684 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12507/12507 [==============================] - 30s 2ms/step - loss: 0.3503 - f1_score_metric: 0.8244 - val_loss: 0.2925 - val_f1_score_metric: 0.9452 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12507/12507 [==============================] - 30s 2ms/step - loss: 0.3457 - f1_score_metric: 0.8269 - val_loss: 0.2902 - val_f1_score_metric: 0.9524 - lr: 9.0484e-04\n",
      "Epoch 12/100\n",
      "12507/12507 [==============================] - 34s 3ms/step - loss: 0.3404 - f1_score_metric: 0.8293 - val_loss: 0.2568 - val_f1_score_metric: 0.9569 - lr: 8.1873e-04\n",
      "Perte finale sur le jeu d'entraînement: 0.34043940901756287\n",
      "Exactitude finale sur le jeu d'entraînement: 0.8292657732963562\n",
      "Perte finale sur le jeu de validation: 0.2567695677280426\n",
      "Exactitude finale sur le jeu de validation: 0.9568954706192017\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 13 -  métrique f1-score avec classifivation binaire (loss=binary_crossentropy) + SMOTE\n",
    "# ------------------------------------------------------------------------------------------\n",
    "#          reseau dense (200 - 100 - 50 - 1)\n",
    "#          activation=reLu (rechercher des relation non linéaires simples)\n",
    "#          metric=f1-score \n",
    "\n",
    "# TEST 13.1 - Construction du modèle\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"relu\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.4)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"relu\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.4)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"relu\", name = \"Dense_3\")\n",
    "Dropout3 = Dropout(0.4)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 13.2 - COMPILATION DU MODELE (classification binaire - SMOTE - f1-score)\n",
    "# --------------------------------\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "# A privilégier pour la classification binaire\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score_metric])\n",
    "\n",
    "\n",
    "# TEST 13.3 - APPRENTISSAGE DU MODELE - classification binaire - f1-Score - SMOTE\n",
    "# ----------------------------------\n",
    "history_test13 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test13.history['loss']\n",
    "train_accuracy_history = history_test13.history['f1_score_metric']\n",
    "\n",
    "# Récupération des scores de validation\n",
    "val_loss_history = history_test13.history.get('val_loss')\n",
    "val_accuracy_history = history_test13.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n",
    "# Sauvegarde du modèle (il sera utilisé pour la partie interprétabilité)\n",
    "model.save(repModels + 'DNNmodel_150_80_40.h5')\n",
    "#model.save(repModels + 'DNNmodel_10_8_6.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2db5679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 14s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88    222340\n",
      "           1       0.50      0.82      0.62     48377\n",
      "\n",
      "    accuracy                           0.82    270717\n",
      "   macro avg       0.73      0.82      0.75    270717\n",
      "weighted avg       0.87      0.82      0.83    270717\n",
      "\n",
      "[[182395  39945]\n",
      " [  8841  39536]]\n",
      "2822/2822 [==============================] - 5s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86     73973\n",
      "           1       0.45      0.73      0.56     16300\n",
      "\n",
      "    accuracy                           0.79     90273\n",
      "   macro avg       0.69      0.77      0.71     90273\n",
      "weighted avg       0.84      0.79      0.81     90273\n",
      "\n",
      "[[59411 14562]\n",
      " [ 4413 11887]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 13.4 - EVALUATION DU MODELE (binary - f1-Score - activation=relu)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = (y_train_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n",
    "# 0.77 - 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6b3bc",
   "metadata": {},
   "source": [
    "<u>TEST 14 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE PONDERE - LOSS=BINARY_CROSSENTROPY - SMOTE</u>\n",
    "activation=tanh\n",
    "réseaux avec plus de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20769236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,801\n",
      "Trainable params: 60,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 49s 3ms/step - loss: 0.4522 - named_weighted_f1_score: 11.3629 - val_loss: 0.5048 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 42s 3ms/step - loss: 0.4367 - named_weighted_f1_score: 11.3621 - val_loss: 0.4810 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 46s 4ms/step - loss: 0.4299 - named_weighted_f1_score: 11.3604 - val_loss: 0.4531 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 44s 4ms/step - loss: 0.4248 - named_weighted_f1_score: 11.3630 - val_loss: 0.4304 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 49s 4ms/step - loss: 0.4200 - named_weighted_f1_score: 11.3615 - val_loss: 0.3961 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12507/12507 [==============================] - 40s 3ms/step - loss: 0.4156 - named_weighted_f1_score: 11.3611 - val_loss: 0.3910 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12507/12507 [==============================] - 45s 4ms/step - loss: 0.4111 - named_weighted_f1_score: 11.3623 - val_loss: 0.3672 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.4072 - named_weighted_f1_score: 11.3614 - val_loss: 0.3957 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.4040 - named_weighted_f1_score: 11.3615 - val_loss: 0.3872 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12507/12507 [==============================] - 33s 3ms/step - loss: 0.3997 - named_weighted_f1_score: 11.3619 - val_loss: 0.3606 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12507/12507 [==============================] - 33s 3ms/step - loss: 0.3957 - named_weighted_f1_score: 11.3626 - val_loss: 0.3444 - val_named_weighted_f1_score: 0.0000e+00 - lr: 9.0484e-04\n",
      "Epoch 12/100\n",
      "12507/12507 [==============================] - 31s 3ms/step - loss: 0.3910 - named_weighted_f1_score: 11.3629 - val_loss: 0.3211 - val_named_weighted_f1_score: 0.0000e+00 - lr: 8.1873e-04\n",
      "Epoch 13/100\n",
      "12507/12507 [==============================] - 31s 3ms/step - loss: 0.3862 - named_weighted_f1_score: 11.3640 - val_loss: 0.3245 - val_named_weighted_f1_score: 0.0000e+00 - lr: 7.4082e-04\n",
      "Epoch 14/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.3828 - named_weighted_f1_score: 11.3627 - val_loss: 0.3410 - val_named_weighted_f1_score: 0.0000e+00 - lr: 6.7032e-04\n",
      "Epoch 15/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.3802 - named_weighted_f1_score: 11.3617 - val_loss: 0.3348 - val_named_weighted_f1_score: 0.0000e+00 - lr: 6.0653e-04\n",
      "Perte finale sur le jeu d'entraînement: 0.3802459239959717\n",
      "Exactitude finale sur le jeu d'entraînement: 11.361659049987793\n",
      "Perte finale sur le jeu de validation: 0.3347755968570709\n",
      "Exactitude finale sur le jeu de validation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 14 -  métrique f1-score pondéré avec classifivation binaire \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 200 - 100 - 50 - 1\n",
    "#          activation=tanh\n",
    "#          metric=f1-score pondéré\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 10.1 - CONSTRUCTION DU MODELE (Classification binaire - SMOTE - f1-Score pondéré)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"tanh\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"tanh\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 14.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Utilisation d'une factory de fonction afin de permettre le passage en paramètre de class_weights\n",
    "def weighted_f1_score(y_true, y_pred, class_weights):\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "\n",
    "    # Convertir les labels en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=len(class_weights))\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=len(class_weights))\n",
    "\n",
    "    # Précision et rappel pour chaque classe\n",
    "    true_positives = K.sum(K.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = K.sum(K.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = K.sum(K.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    # F1-score par classe\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    # Conversion des poids en float32\n",
    "    class_weights = K.cast(class_weights, dtype='float32')\n",
    "\n",
    "    # Calcul du f1-score pondéré\n",
    "    weighted_f1 = K.sum(f1_scores * class_weights) / K.sum(class_weights)\n",
    "\n",
    "    return weighted_f1\n",
    "\n",
    "\n",
    "# Calcul des poids des classes\n",
    "unique_classes, class_counts = np.unique(y_train_binary_resampled, return_counts=True)\n",
    "class_weights = class_counts / len(y_train_binary_resampled)\n",
    "class_weights = 1.0 / class_weights\n",
    "\n",
    "# Création de la fonction weighted à partir de la factory afin de passer 2 des arguments lors de l'appel dans le compile, le 3ème (class_weights étant prés transmis via la factory\n",
    "def make_weighted_f1_score(class_weights):\n",
    "    def named_weighted_f1_score(y_true, y_pred):\n",
    "        return weighted_f1_score(y_true, y_pred, class_weights)\n",
    "    return named_weighted_f1_score\n",
    "\n",
    "weighted_f1 = make_weighted_f1_score(class_weights)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[weighted_f1])\n",
    "              #metrics=[f1_score_metric])  \n",
    "\n",
    "# TEST 14.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test14 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test14.history['loss']\n",
    "train_accuracy_history = history_test14.history['named_weighted_f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "#train_accuracy_history = history_test5.history['f1_score_metric']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test14.history.get('val_loss')\n",
    "val_accuracy_history = history_test14.history.get('val_named_weighted_f1_score')\n",
    "#val_accuracy_history = history_test5.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f810027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 19s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.80      0.87    222340\n",
      "           1       0.46      0.81      0.59     48377\n",
      "\n",
      "    accuracy                           0.80    270717\n",
      "   macro avg       0.71      0.80      0.73    270717\n",
      "weighted avg       0.86      0.80      0.82    270717\n",
      "\n",
      "[[177197  45143]\n",
      " [  9156  39221]]\n",
      "2822/2822 [==============================] - 5s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86     73973\n",
      "           1       0.45      0.77      0.57     16300\n",
      "\n",
      "    accuracy                           0.79     90273\n",
      "   macro avg       0.69      0.78      0.71     90273\n",
      "weighted avg       0.85      0.79      0.80     90273\n",
      "\n",
      "[[58267 15706]\n",
      " [ 3695 12605]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 14.4 - EVALUATION DU MODELE (binary - f1-Score pondéré - activation=tanh)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = (y_train_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n",
    "# 0.77 - 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cad80b",
   "metadata": {},
   "source": [
    "<u>TEST 15 - CLASSIFICATION BINAIRE - METRIQUE=F1-SCORE PONDERE - LOSS=BINARY_CROSSENTROPY - SMOTE</u>\n",
    "activation=relu\n",
    "réseaux avec plus de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d72efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,801\n",
      "Trainable params: 60,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "12507/12507 [==============================] - 45s 3ms/step - loss: 0.4268 - named_weighted_f1_score: 11.3621 - val_loss: 0.4025 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.3841 - named_weighted_f1_score: 11.3615 - val_loss: 0.3092 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.3621 - named_weighted_f1_score: 11.3626 - val_loss: 0.2855 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.3471 - named_weighted_f1_score: 11.3612 - val_loss: 0.2663 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.3366 - named_weighted_f1_score: 11.3623 - val_loss: 0.2597 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.3271 - named_weighted_f1_score: 11.3611 - val_loss: 0.2905 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.3197 - named_weighted_f1_score: 11.3631 - val_loss: 0.2388 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "12507/12507 [==============================] - 30s 2ms/step - loss: 0.3139 - named_weighted_f1_score: 11.3626 - val_loss: 0.1979 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.3082 - named_weighted_f1_score: 11.3636 - val_loss: 0.1763 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "12507/12507 [==============================] - 39s 3ms/step - loss: 0.3051 - named_weighted_f1_score: 11.3632 - val_loss: 0.1894 - val_named_weighted_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "12507/12507 [==============================] - 39s 3ms/step - loss: 0.2985 - named_weighted_f1_score: 11.3621 - val_loss: 0.2214 - val_named_weighted_f1_score: 0.0000e+00 - lr: 9.0484e-04\n",
      "Epoch 12/100\n",
      "12507/12507 [==============================] - 36s 3ms/step - loss: 0.2923 - named_weighted_f1_score: 11.3622 - val_loss: 0.1978 - val_named_weighted_f1_score: 0.0000e+00 - lr: 8.1873e-04\n",
      "Perte finale sur le jeu d'entraînement: 0.2922647297382355\n",
      "Exactitude finale sur le jeu d'entraînement: 11.362180709838867\n",
      "Perte finale sur le jeu de validation: 0.19783613085746765\n",
      "Exactitude finale sur le jeu de validation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TEST 15 -  métrique f1-score pondéré avec classifivation binaire \n",
    "#          (loss=sparse_categorical_crossentropy) + SMOTE\n",
    "#          Couches avec 200 - 100 - 50 - 1\n",
    "#          activation=relu\n",
    "#          métric=f1-score pondéré\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# TEST 15.1 - CONSTRUCTION DU MODELE (Classification binaire - SMOTE - f1-Score pondéré)\n",
    "# ---------------------------------\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"relu\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.2)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"relu\", name = \"Dense_2\")\n",
    "dropout2 = Dropout(0.2)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"relu\", name = \"Dense_3\")\n",
    "dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# TEST 15.2 - COMPILATION DU MODELE (Classification multiclasse - SMOTE - métrique f1-score pondéré\n",
    "# ------------------------------\n",
    "# Utilisation d'une factory de fonction afin de permettre le passage en paramètre de class_weights\n",
    "def weighted_f1_score(y_true, y_pred, class_weights):\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "\n",
    "    # Convertir les labels en one-hot encoding\n",
    "    y_true_one_hot = tf.one_hot(y_true, depth=len(class_weights))\n",
    "    y_pred_one_hot = tf.one_hot(y_pred, depth=len(class_weights))\n",
    "\n",
    "    # Précision et rappel pour chaque classe\n",
    "    true_positives = K.sum(K.cast(y_true_one_hot * y_pred_one_hot, 'float32'), axis=0)\n",
    "    predicted_positives = K.sum(K.cast(y_pred_one_hot, 'float32'), axis=0)\n",
    "    possible_positives = K.sum(K.cast(y_true_one_hot, 'float32'), axis=0)\n",
    "\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "    # F1-score par classe\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    # Conversion des poids en float32\n",
    "    class_weights = K.cast(class_weights, dtype='float32')\n",
    "\n",
    "    # Calcul du f1-score pondéré\n",
    "    weighted_f1 = K.sum(f1_scores * class_weights) / K.sum(class_weights)\n",
    "\n",
    "    return weighted_f1\n",
    "\n",
    "\n",
    "# Calcul des poids des classes\n",
    "unique_classes, class_counts = np.unique(y_train_binary_resampled, return_counts=True)\n",
    "class_weights = class_counts / len(y_train_binary_resampled)\n",
    "class_weights = 1.0 / class_weights\n",
    "\n",
    "# Création de la fonction weighted à partir de la factory afin de passer 2 des arguments lors de l'appel dans le compile, le 3ème (class_weights étant prés transmis via la factory\n",
    "def make_weighted_f1_score(class_weights):\n",
    "    def named_weighted_f1_score(y_true, y_pred):\n",
    "        return weighted_f1_score(y_true, y_pred, class_weights)\n",
    "    return named_weighted_f1_score\n",
    "\n",
    "weighted_f1 = make_weighted_f1_score(class_weights)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[weighted_f1])\n",
    "              #metrics=[f1_score_metric])  \n",
    "\n",
    "# TEST 15.3 - APPRENTISSAGE DU MODELE (multiclasse - SMOTE - f1-Score pondéré)\n",
    "# -----------------------------------\n",
    "history_test15 = model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "\n",
    "# Récupération des pertes et des précisions (ou autres métriques) pour chaque époque\n",
    "train_loss_history = history_test15.history['loss']\n",
    "train_accuracy_history = history_test15.history['named_weighted_f1_score']                   # et pas weighted_f1 a cause du factory\n",
    "#train_accuracy_history = history_test5.history['f1_score_metric']                   # et pas weighted_f1 a cause du factory\n",
    "\n",
    "# Si vous avez utilisé validation_data ou validation_split, vous pouvez également obtenir les scores de validation\n",
    "val_loss_history = history_test15.history.get('val_loss')\n",
    "val_accuracy_history = history_test15.history.get('val_named_weighted_f1_score')\n",
    "#val_accuracy_history = history_test5.history.get('val_f1_score_metric')\n",
    "\n",
    "print(f\"Perte finale sur le jeu d'entraînement: {train_loss_history[-1]}\")\n",
    "print(f\"Exactitude finale sur le jeu d'entraînement: {train_accuracy_history[-1]}\")\n",
    "if val_loss_history and val_accuracy_history:\n",
    "    print(f\"Perte finale sur le jeu de validation: {val_loss_history[-1]}\")\n",
    "    print(f\"Exactitude finale sur le jeu de validation: {val_accuracy_history[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39111922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8460/8460 [==============================] - 12s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90    222340\n",
      "           1       0.55      0.79      0.65     48377\n",
      "\n",
      "    accuracy                           0.85    270717\n",
      "   macro avg       0.75      0.82      0.78    270717\n",
      "weighted avg       0.88      0.85      0.86    270717\n",
      "\n",
      "[[191420  30920]\n",
      " [ 10220  38157]]\n",
      "2822/2822 [==============================] - 3s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88     73973\n",
      "           1       0.47      0.67      0.56     16300\n",
      "\n",
      "    accuracy                           0.81     90273\n",
      "   macro avg       0.70      0.75      0.72     90273\n",
      "weighted avg       0.84      0.81      0.82     90273\n",
      "\n",
      "[[61837 12136]\n",
      " [ 5366 10934]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 15.4 - EVALUATION DU MODELE (binary - f1-Score pondéré - activation=relu)\n",
    "# -------------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "threshold = 0.5\n",
    "y_train_pred=model.predict(X_train)\n",
    "y_train_pred_class = (y_train_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_train_binary, y_train_pred_class))\n",
    "print(confusion_matrix(y_train_binary, y_train_pred_class))\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, y_pred_class))\n",
    "print(confusion_matrix(y_test_binary,y_pred_class))\n",
    "\n",
    "# 0.77 - 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81de62c",
   "metadata": {},
   "source": [
    "### <u>TEST 16 - DNN + XGBoost - méthode Stacking - Poids équivalent classe 0 et 1</u>\n",
    "poids iso entre classe 0 et 1 (non pondéré)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7987ceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,801\n",
      "Trainable params: 60,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12507/12507 [==============================] - 42s 3ms/step - loss: 0.4595 - f1_score_metric: 0.7515 - val_loss: 0.4893 - val_f1_score_metric: 0.8862 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.4451 - f1_score_metric: 0.7620 - val_loss: 0.4092 - val_f1_score_metric: 0.9205 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.4416 - f1_score_metric: 0.7643 - val_loss: 0.4093 - val_f1_score_metric: 0.9103 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "12507/12507 [==============================] - 30s 2ms/step - loss: 0.4385 - f1_score_metric: 0.7665 - val_loss: 0.4408 - val_f1_score_metric: 0.9042 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.4363 - f1_score_metric: 0.7678 - val_loss: 0.4403 - val_f1_score_metric: 0.9070 - lr: 0.0010\n",
      "13897/13897 [==============================] - 17s 1ms/step\n",
      "2822/2822 [==============================] - 4s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "''' \n",
    "Surapprentissage probable (0.2 point d'écart entre train et test) malgré l'utilisation d'un earlystopping\n",
    "Amélioration : \n",
    "1. en ajoutant du dropout (passage de 0.2 --> 0.4)\n",
    "1.Bis diminuer le nb de neurones\n",
    "2. Régularisation L2/L1 sur dnn / XGBoost / REgression Logistique\n",
    "3. Réduction du surajustement du xgboost\n",
    "4. Remplacer la RL par un decision tree \n",
    "'''\n",
    "# TEST 16.1 - Constrution du modèle DNN\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.4)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"tanh\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.4)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"tanh\", name = \"Dense_3\")\n",
    "#Dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "# Construction et compilation du DNN\n",
    "dnn_xgboost_model = Model(inputs = inputs, outputs = outputs)\n",
    "dnn_xgboost_model.summary()\n",
    "dnn_xgboost_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score_metric])\n",
    "\n",
    "# Train DNN model\n",
    "class_weight = {0: 1, 1: 1}             # Trois fois plus de poids à la classe 1\n",
    "dnn_xgboost_model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "dnn_predictions_train = dnn_xgboost_model.predict(X_train_binary_resampled)\n",
    "dnn_predictions_test = dnn_xgboost_model.predict(X_test)\n",
    "\n",
    "# Train XGBoost model   # Optimisation du XGBoost pour limiter le sureapprentissage\n",
    "xgb_model = xgb.XGBClassifier(max_depth=4, min_child_weight=10, gamma=0.1, alpha=0.5)\n",
    "xgb_model.fit(X_train_binary_resampled, y_train_binary_resampled)\n",
    "xgb_predictions_train = xgb_model.predict(X_train_binary_resampled)\n",
    "xgb_predictions_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Stacking: Use predictions as new features\n",
    "stacked_train = np.column_stack((dnn_predictions_train, xgb_predictions_train))\n",
    "stacked_test = np.column_stack((dnn_predictions_test, xgb_predictions_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba146fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90    222340\n",
      "           1       0.92      0.87      0.90    222340\n",
      "\n",
      "    accuracy                           0.90    444680\n",
      "   macro avg       0.90      0.90      0.90    444680\n",
      "weighted avg       0.90      0.90      0.90    444680\n",
      "\n",
      "[[206414  15926]\n",
      " [ 28072 194268]]\n",
      "Stacked model accuracy: 0.8470306736233425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     73973\n",
      "           1       0.59      0.48      0.53     16300\n",
      "\n",
      "    accuracy                           0.85     90273\n",
      "   macro avg       0.74      0.70      0.72     90273\n",
      "weighted avg       0.84      0.85      0.84     90273\n",
      "\n",
      "[[68648  5325]\n",
      " [ 8484  7816]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 16.4 - EVALUATION DU MODELE - DNN + XGBOOST - SMOTE - aucun poids\n",
    "# -----------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a logistic regression as a meta-model\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacked_train, y_train_binary_resampled)\n",
    "\n",
    "# Evaluation Train\n",
    "final_predictions = meta_model.predict(stacked_train)\n",
    "accuracy = accuracy_score(y_train_binary, final_predictions)\n",
    "print(f\"Stacked model accuracy (train) : {accuracy}\")\n",
    "print(classification_report(y_train_binary_resampled, final_predictions))\n",
    "print(confusion_matrix(y_train_binary_resampled, final_predictions))\n",
    "\n",
    "# EValuation test\n",
    "final_predictions = meta_model.predict(stacked_test)\n",
    "accuracy = accuracy_score(y_test_binary, final_predictions)\n",
    "print(f\"Stacked model accuracy (test): {accuracy}\")\n",
    "\n",
    "# EVALUATION DU MODELE sur échantillon test\n",
    "#y_pred=model.predict(X_test)\n",
    "#threshold = 0.5\n",
    "#y_pred_class = (y_pred >= threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, final_predictions))\n",
    "print(confusion_matrix(y_test_binary, final_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7c6b4",
   "metadata": {},
   "source": [
    "### <u>TEST 17 - DNN + XGBoost - méthode Stacking - Poids sur classe 1</u>\n",
    "poids supérieur sur classe 1 (pondération)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68fcc6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 177)]             0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 200)               35600     \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " Dense_4 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,801\n",
      "Trainable params: 60,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "12507/12507 [==============================] - 49s 4ms/step - loss: 0.7597 - f1_score_metric: 0.7536 - val_loss: 0.2214 - val_f1_score_metric: 0.9646 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "12507/12507 [==============================] - 34s 3ms/step - loss: 0.7358 - f1_score_metric: 0.7612 - val_loss: 0.1970 - val_f1_score_metric: 0.9731 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "12507/12507 [==============================] - 32s 3ms/step - loss: 0.7297 - f1_score_metric: 0.7635 - val_loss: 0.1857 - val_f1_score_metric: 0.9741 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "12507/12507 [==============================] - 31s 2ms/step - loss: 0.7245 - f1_score_metric: 0.7658 - val_loss: 0.1934 - val_f1_score_metric: 0.9731 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "12507/12507 [==============================] - 41s 3ms/step - loss: 0.7202 - f1_score_metric: 0.7668 - val_loss: 0.1984 - val_f1_score_metric: 0.9730 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "12507/12507 [==============================] - 44s 3ms/step - loss: 0.7164 - f1_score_metric: 0.7679 - val_loss: 0.1982 - val_f1_score_metric: 0.9706 - lr: 0.0010\n",
      "13897/13897 [==============================] - 31s 2ms/step\n",
      "2822/2822 [==============================] - 7s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "''' \n",
    "Surapprentissage probable (0.2 point d'écart entre train et test) malgré l'utilisation d'un earlystopping\n",
    "Amélioration : \n",
    "1. en ajoutant du dropout (passage de 0.2 --> 0.4)\n",
    "1.Bis diminuer le nb de neurones\n",
    "2. Régularisation L2/L1 sur dnn / XGBoost / REgression Logistique\n",
    "3. Réduction du surajustement du xgboost\n",
    "4. Remplacer la RL par un decision tree \n",
    "'''\n",
    "# TEST 17.1 - Constrution du modèle DNN\n",
    "inputs = Input(shape = (X_train.shape[1]), name = \"Input\")\n",
    "\n",
    "dense1 = Dense(units = 200, activation = \"tanh\", name = \"Dense_1\")\n",
    "dropout1 = Dropout(0.4)\n",
    "\n",
    "dense2 = Dense(units = 100, activation = \"tanh\", name = \"Dense_2\")\n",
    "Dropout2 = Dropout(0.4)\n",
    "\n",
    "dense3 = Dense(units = 50, activation = \"tanh\", name = \"Dense_3\")\n",
    "#Dropout3 = Dropout(0.2)\n",
    "dense4 = Dense(units = 1, activation = \"sigmoid\", name = \"Dense_4\")\n",
    "\n",
    "\n",
    "# Construction fonctionnelle du réseau\n",
    "x=dense1(inputs)\n",
    "x=dropout1(x)\n",
    "x=dense2(x)\n",
    "x=Dropout2(x)\n",
    "x=dense3(x)\n",
    "#x=Dropout3(x)\n",
    "outputs=dense4(x)\n",
    "\n",
    "# Construction et compilation du DNN\n",
    "dnn_xgboost_model = Model(inputs = inputs, outputs = outputs)\n",
    "dnn_xgboost_model.summary()\n",
    "dnn_xgboost_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score_metric])\n",
    "\n",
    "# Train DNN model\n",
    "class_weight = {0: 1, 1: 3}             # Trois fois plus de poids à la classe 1\n",
    "dnn_xgboost_model.fit(\n",
    "    X_train_binary_resampled, \n",
    "    y_train_binary_resampled, \n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight, \n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler],\n",
    ")\n",
    "dnn_predictions_train = dnn_xgboost_model.predict(X_train_binary_resampled)\n",
    "dnn_predictions_test = dnn_xgboost_model.predict(X_test)\n",
    "\n",
    "# Train XGBoost model   # Optimisation du XGBoost pour limiter le sureapprentissage\n",
    "xgb_model = xgb.XGBClassifier(max_depth=4, min_child_weight=10, gamma=0.1, alpha=0.5)\n",
    "xgb_model.fit(X_train_binary_resampled, y_train_binary_resampled)\n",
    "xgb_predictions_train = xgb_model.predict(X_train_binary_resampled)\n",
    "xgb_predictions_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Stacking: Use predictions as new features\n",
    "stacked_train = np.column_stack((dnn_predictions_train, xgb_predictions_train))\n",
    "stacked_test = np.column_stack((dnn_predictions_test, xgb_predictions_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac648af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90    222340\n",
      "           1       0.92      0.87      0.90    222340\n",
      "\n",
      "    accuracy                           0.90    444680\n",
      "   macro avg       0.90      0.90      0.90    444680\n",
      "weighted avg       0.90      0.90      0.90    444680\n",
      "\n",
      "[[206414  15926]\n",
      " [ 28072 194268]]\n",
      "Stacked model accuracy: 0.8470306736233425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     73973\n",
      "           1       0.59      0.48      0.53     16300\n",
      "\n",
      "    accuracy                           0.85     90273\n",
      "   macro avg       0.74      0.70      0.72     90273\n",
      "weighted avg       0.84      0.85      0.84     90273\n",
      "\n",
      "[[68648  5325]\n",
      " [ 8484  7816]]\n"
     ]
    }
   ],
   "source": [
    "# TEST 17.4 - EVALUATION DU MODELE - DNN + XGBOOST - SMOTE - poids sur classe 1\n",
    "# -----------------------------\n",
    "# Evaluation des performances du modèle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a logistic regression as a meta-model\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(stacked_train, y_train_binary_resampled)\n",
    "\n",
    "# Evaluation Train\n",
    "final_predictions = meta_model.predict(stacked_train)\n",
    "print(classification_report(y_train_binary_resampled, final_predictions))\n",
    "print(confusion_matrix(y_train_binary_resampled, final_predictions))\n",
    "\n",
    "# EValuation test\n",
    "final_predictions = meta_model.predict(stacked_test)\n",
    "accuracy = accuracy_score(y_test_binary, final_predictions)\n",
    "print(f\"Stacked model accuracy: {accuracy}\")\n",
    "\n",
    "# EVALUATION DU MODELE sur échantillon test\n",
    "#y_pred=model.predict(X_test)\n",
    "#threshold = 0.5\n",
    "#y_pred_class = (y_pred >= threshold).astype(int)\n",
    "\n",
    "# Matrice de confusion\n",
    "print(classification_report(y_test_binary, final_predictions))\n",
    "print(confusion_matrix(y_test_binary, final_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27098d4b",
   "metadata": {},
   "source": [
    "### ANNEXES - A TESTER EN COMPLEMENT (RAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAIT : Fonction de callback à ajouter\n",
    "# Sauvegarder le modèle pour relancer l'apprentissage à partir de la dernière sauvegarde\n",
    "# FAIT Ajouter un modèle à la fin du réseau de neurone : xgboost par exemple ou random Forest\n",
    "# FAIT : F1-score à privilégier afin d'équilibrer la précision et le rappel\n",
    "# FAIT : Faut-il rééquilibrer les données y compris en réseau de neurones\n",
    "\n",
    "# FAIT : Refaire un test sur target binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab474c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87     73973\n",
      "           1       0.44      0.49      0.46     16300\n",
      "\n",
      "    accuracy                           0.80     90273\n",
      "   macro avg       0.66      0.68      0.67     90273\n",
      "weighted avg       0.80      0.80      0.80     90273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# En complément des DNN --> tester les résultats de ce modèle\n",
    "# Test modèle MLPClassifier (en marge des réseaux de neurones) \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Entraîner le modèle\n",
    "model = MLPClassifier(hidden_layer_sizes=(200, 100, 50), activation='relu', solver='adam', max_iter=300)\n",
    "model.fit(X_train_binary_resampled, y_train_binary_resampled)\n",
    "\n",
    "# Prédire et évaluer\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test_binary, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bf1041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98    222340\n",
      "           1       0.88      0.93      0.91     48377\n",
      "\n",
      "    accuracy                           0.97    270717\n",
      "   macro avg       0.93      0.95      0.94    270717\n",
      "weighted avg       0.97      0.97      0.97    270717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prédire et évaluer\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(classification_report(y_train_binary, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb84ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet_accident",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
